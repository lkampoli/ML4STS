{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting \n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor, RegressorChain\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn import kernel_ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.utils.fixes import loguniform\n",
    "\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.python.keras.models import Sequential\n",
    "#from tensorflow.python.keras.layers import Dense, Dropout\n",
    "#from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "#from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#from livelossplot import PlotLossesKeras\n",
    "#from keras.callbacks import TensorBoard\n",
    "#from keras.utils.vis_utils import plot_model\n",
    "#from keras.models import load_model\n",
    "#from ann_visualizer.visualize import ann_viz;\n",
    "#from keras.models import model_from_json\n",
    "#from keras_sequential_ascii import keras2ascii\n",
    "#from keras.optimizers import SGD, Adam, RMSprop, Adagrad\n",
    "#from keras import regularizers\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor\n",
    "# criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "# max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, ccp_alpha=0.0\n",
    "def est_DT():\n",
    "    hp = [{'criterion': ('mse', 'friedman_mse', 'mae'), \n",
    "           'splitter': ('best', 'random'),             \n",
    "           'max_features': ('auto', 'sqrt', 'log2'),  \n",
    "    }]\n",
    "    est = DecisionTreeRegressor()\n",
    "    return est, hp\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html?highlight=extra%20tree#sklearn.ensemble.ExtraTreesRegressor\n",
    "# n_estimators=100, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto',\n",
    "# max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, \n",
    "# verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None\n",
    "def est_ET():\n",
    "    hp = [{'n_estimators': (1, 100,),\n",
    "           'min_weight_fraction_leaf': (0.0, 0.25, 0.5,),\n",
    "           'max_features': ('sqrt','log2','auto', None,),\n",
    "           'max_samples': loguniform(1, 1000),\n",
    "           'bootstrap': (True, False,),\n",
    "           'oob_score': (True, False,),\n",
    "           'warm_start': (True, False,),\n",
    "           'criterion': ('mse', 'mae',),\n",
    "           'max_depth': (1,10,100,None,),\n",
    "           'max_leaf_nodes': (2, 100,),\n",
    "           'min_samples_split': (10,),\n",
    "           'min_samples_leaf': loguniform(1, 100),\n",
    "    }]\n",
    "    est = ensemble.ExtraTreesRegressor()\n",
    "    #regr = MultiOutputRegressor(estimator=est)\n",
    "    return est, hp\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html?highlight=gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor\n",
    "# loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, \n",
    "# min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, \n",
    "# max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, \n",
    "# tol=0.0001, ccp_alpha=0.0\n",
    "def est_GB():\n",
    "    hp = [{'n_estimators': (10, 100, 1000,),\n",
    "           'min_weight_fraction_leaf': (0.0, 0.1, 0.2, 0.3,),\n",
    "           'max_features': ('sqrt', 'log2', 'auto',),\n",
    "           'warm_start': (False, True),\n",
    "           'criterion': ('friedman_mse', 'mse', 'mae',),\n",
    "           'max_depth': (1, 10, 100, None,),\n",
    "           'min_samples_split': (2, 5, 10,),\n",
    "           'min_samples_leaf': (2, 5, 10,),\n",
    "           'loss': ('ls', 'lad', 'huber', 'quantile',),\n",
    "    }]\n",
    "    est = ensemble.GradientBoostingRegressor()\n",
    "    return est, hp\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor\n",
    "# loss='least_squares', learning_rate=0.1, max_iter=100, max_leaf_nodes=31, max_depth=None, min_samples_leaf=20, l2_regularization=0.0, \n",
    "# max_bins=255, categorical_features=None, monotonic_cst=None, warm_start=False, early_stopping='auto', scoring='loss', \n",
    "# validation_fraction=0.1, n_iter_no_change=10, tol=1e-07, verbose=0, random_state=None\n",
    "def est_HGB(est):\n",
    "    hp = [{'warm_start': (False, True),\n",
    "           'max_depth': (1, 10, 100, None,),\n",
    "           'min_samples_leaf': (2, 5, 10,),\n",
    "           'loss': ('ls', 'lad', 'huber', 'quantile',),\n",
    "           'max_leaf_nodes': (2, 10, 20, 30, 40, 50, 100,),\n",
    "    }]\n",
    "    est = ensemble.HistGradientBoostingRegressor()\n",
    "    return est, hp\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor\n",
    "# n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None\n",
    "def est_KN():\n",
    "    hp = [{'algorithm': ('ball_tree', 'kd_tree', 'brute', 'auto',),\n",
    "           'n_neighbors': (1, 5, 10, 20),\n",
    "           'leaf_size': (1, 10, 50, 100,),\n",
    "           'weights': ('uniform', 'distance',),\n",
    "#          'metric': ('minkowski', ),  \n",
    "#          'metric_params': (), \n",
    "           'p': (1, 2,),\n",
    "    }]\n",
    "    est = neighbors.KNeighborsRegressor()\n",
    "    return est, hp\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge\n",
    "# alpha=1, *, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None\n",
    "def est_KR():\n",
    "    hp = [{'kernel': ('poly', 'rbf',),\n",
    "           'alpha': (1e-3, 1e-2, 1e-1, 0.0, 0.5, 1.,),\n",
    "#          'degree': (),\n",
    "#          'coef0': (),\n",
    "           'gamma': (0.1, 1, 2,),\n",
    "    }]\n",
    "    est = kernel_ridge.KernelRidge()\n",
    "    return est, hp\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html?highlight=mlp#sklearn.neural_network.MLPRegressor\n",
    "# hidden_layer_sizes=100, activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', \n",
    "# learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, \n",
    "# warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, \n",
    "# beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000\n",
    "def est_MLP(est):\n",
    "    hp = [{'hidden_layer_sizes': (10, 50, 100, 150, 200,),\n",
    "                    'activation' : ('tanh', 'relu',),\n",
    "                    'solver' : ('lbfgs','adam','sgd',), \n",
    "                    'learning_rate' : ('constant', 'invscaling', 'adaptive',),\n",
    "                    'nesterovs_momentum': (True, False,),\n",
    "                    'alpha': (0.00001, 0.0001, 0.001, 0.01, 0.1, 0.0,),\n",
    "                    'warm_start': (True, False,),\n",
    "                    'early_stopping': (True, False,),\n",
    "                    'max_iter': (1000,)\n",
    "    }]\n",
    "    est = MLPRegressor()\n",
    "    return est, hp\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "# n_estimators=100, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "# min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, \n",
    "# verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None\n",
    "def est_RF():\n",
    "    hp = [{'n_estimators': (1, 50, 100,),\n",
    "           'min_weight_fraction_leaf': (0.0, 0.25, 0.5,),\n",
    "           'max_features': ('sqrt', 'log2', 'auto',),\n",
    "#          'bootstrap': (True, False,),\n",
    "#          'oob_score': (True, False,),\n",
    "#          'warm_start': (True, False,),\n",
    "           'criterion': ('mse', 'mae',),\n",
    "           'max_depth': (1, 10, 100,),\n",
    "           'max_leaf_nodes': (2, 100,),\n",
    "           'min_samples_split': (2, 5, 10,),\n",
    "           'min_impurity_decrease': (0.1, 0.2, 0.3,),\n",
    "           'min_samples_leaf': (1, 10, 100,),\n",
    "#          'min_impurity_split':= (), \n",
    "#          'ccp_alpha': (),\n",
    "#          'max_samples': (),\n",
    "    }]\n",
    "    est = ensemble.RandomForestRegressor(random_state=69)\n",
    "    return est, hp\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR\n",
    "# kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1\n",
    "def est_SVM():\n",
    "    \"\"\" \"\"\"\n",
    "    hp = [{'kernel': ('poly', 'rbf',),\n",
    "           'gamma': ('scale', 'auto',),\n",
    "           'C': (1e-1, 1e0, 1e1,),\n",
    "           'epsilon': (1e-2, 1e-1, 1e0, 1e1,),\n",
    "           'coef0': (0.0, 0.1, 0.2,),\n",
    "    }]\n",
    "    est = svm.SVR()\n",
    "    return est, hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def mk_tree(filename, parent_dir, process, algorithm):\n",
    "    \n",
    "#    if (process == \"DR\"):    \n",
    "#        data = filename[18:36]\n",
    "#        dir  = data[9:14]  \n",
    "#        proc = data[15:18] # dis, rec\n",
    "#    elif (process == \"VT\"):    \n",
    "#        data = filename[18:35]\n",
    "#        dir  = data[9:14]  # N2-N2\n",
    "#        proc = data[15:17] # down, up\n",
    "#    elif (process == \"VV\"):#TODO\n",
    "#        data = filename[18:35]\n",
    "#        dir  = data[9:14]  # N2-N2\n",
    "#        proc = data[15:17] # down, up\n",
    "#    elif (process == \"VV2\"):#TODO\n",
    "#        data = filename[18:35]\n",
    "#        dir  = data[9:14]  # N2-N2\n",
    "#        proc = data[15:17] # down, up\n",
    "#    elif (process == \"ZR\"):#TODO\n",
    "#        data = filename[18:35]\n",
    "#        dir  = data[9:14]  # N2-N2\n",
    "#        proc = data[15:17] # down, up\n",
    "#    else:\n",
    "#        print(\"Process not accounted for ... !\")\n",
    "\n",
    "    proc = \"_\" \n",
    "    dir  = \"_\" \n",
    "    proc = \"_\" \n",
    "\n",
    "    #import numpy as np\n",
    "    #dataset_k = np.loadtxt(parent_dir+\"/\"+process[0]+filename)\n",
    "    #print(dataset_k)\n",
    "\n",
    "    # Get the filename only from the initial file path.\n",
    "    data = os.path.basename(filename)\n",
    "    data = os.path.splitext(data)[0]\n",
    "    # Use splitext() to get filename and extension separately.\n",
    "    #(file, ext) = os.path.splitext(filename)\n",
    "    #file_name = Path(file_path).stem\n",
    "\n",
    "    print(\"Dataset: \", data)\n",
    "    print(\"Folder: \", dir)\n",
    "    print(\"Process: \", proc)\n",
    "    print(\"parent_dir: \", parent_dir)\n",
    "\n",
    "    models  = \"models\"\n",
    "    scalers = \"scalers\"\n",
    "    figures = \"figures\"\n",
    "\n",
    "    model  = os.path.join(parent_dir+\"/\"+process+\"/\"+algorithm+\"/\"+data, models)\n",
    "    scaler = os.path.join(parent_dir+\"/\"+process+\"/\"+algorithm+\"/\"+data, scalers)\n",
    "    figure = os.path.join(parent_dir+\"/\"+process+\"/\"+algorithm+\"/\"+data, figures)\n",
    "\n",
    "    print(model)\n",
    "    print(scaler)\n",
    "    print(figure)\n",
    "\n",
    "    shutil.rmtree(data,   ignore_errors=True)\n",
    "    shutil.rmtree(model,  ignore_errors=True)\n",
    "    shutil.rmtree(scaler, ignore_errors=True)\n",
    "    shutil.rmtree(figure, ignore_errors=True)\n",
    "\n",
    "    print(\"Model: \", model)\n",
    "    print(\"Scaler: \", scaler)\n",
    "    print(\"Figure: \", figure)\n",
    "\n",
    "    Path(parent_dir+\"/\"+process+\"/\"+algorithm+\"/\"+data).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    os.mkdir(model)\n",
    "    os.mkdir(scaler)\n",
    "    os.mkdir(figure)\n",
    "\n",
    "    print(\"Directory '%s' created\" %models)\n",
    "    print(\"Directory '%s' created\" %scalers)\n",
    "    print(\"Directory '%s' created\" %figures)\n",
    "\n",
    "    return data, dir, proc, model, scaler, figure\n",
    "\n",
    "\n",
    "def fit(x,y,gs):\n",
    "   t0 = time.time()\n",
    "   gs.fit(x, y)\n",
    "   runtime = time.time() - t0\n",
    "   print(\"Training time: %.6f s\" % runtime)\n",
    "\n",
    "   return gs\n",
    "\n",
    "\n",
    "def predict(x_test, gs):\n",
    "\n",
    "   t0 = time.time()\n",
    "   y_regr = gs.predict(x_test)\n",
    "   regr_predict = time.time() - t0\n",
    "   print(\"Prediction for %d inputs in %.6f s\" % (x_test.shape[0], regr_predict))\n",
    "\n",
    "   return y_regr\n",
    "\n",
    "\n",
    "def scores(sc_x, sc_y, x_train, y_train, x_test, y_test, data, gs):\n",
    "\n",
    "   train_score_mse   = mean_squared_error(      sc_y.inverse_transform(y_train), sc_y.inverse_transform(gs.predict(x_train)))\n",
    "   train_score_mae   = mean_absolute_error(     sc_y.inverse_transform(y_train), sc_y.inverse_transform(gs.predict(x_train)))\n",
    "   train_score_evs   = explained_variance_score(sc_y.inverse_transform(y_train), sc_y.inverse_transform(gs.predict(x_train)))\n",
    "   #train_score_me   = max_error(               sc_y.inverse_transform(y_train), sc_y.inverse_transform(gs.predict(x_train)))\n",
    "   #train_score_msle = mean_squared_log_error(  sc_y.inverse_transform(y_train), sc_y.inverse_transform(gs.predict(x_train)))\n",
    "   train_score_r2    = r2_score(                sc_y.inverse_transform(y_train), sc_y.inverse_transform(gs.predict(x_train)))\n",
    "   \n",
    "   test_score_mse   = mean_squared_error(      sc_y.inverse_transform(y_test), sc_y.inverse_transform(gs.predict(x_test)))\n",
    "   test_score_mae   = mean_absolute_error(     sc_y.inverse_transform(y_test), sc_y.inverse_transform(gs.predict(x_test)))\n",
    "   test_score_evs   = explained_variance_score(sc_y.inverse_transform(y_test), sc_y.inverse_transform(gs.predict(x_test)))\n",
    "   #test_score_me   = max_error(               sc_y.inverse_transform(y_test), sc_y.inverse_transform(gs.predict(x_test)))\n",
    "   #test_score_msle = mean_squared_log_error(  sc_y.inverse_transform(y_test), sc_y.inverse_transform(gs.predict(x_test)))\n",
    "   test_score_r2    = r2_score(                sc_y.inverse_transform(y_test), sc_y.inverse_transform(gs.predict(x_test)))\n",
    "   \n",
    "   \n",
    "   print()\n",
    "   print(\"The model performance for training set\")\n",
    "   print(\"--------------------------------------\")\n",
    "   print('MAE is      {}'.format(train_score_mae ))\n",
    "   print('MSE is      {}'.format(train_score_mse ))\n",
    "   print('EVS is      {}'.format(train_score_evs ))\n",
    "   #print('ME is      {}'.format(train_score_me  ))\n",
    "   #print('MSLE is    {}'.format(train_score_msle))\n",
    "   print('R2 score is {}'.format(train_score_r2  ))\n",
    "   print()\n",
    "   print(\"The model performance for testing set\" )\n",
    "   print(\"--------------------------------------\")\n",
    "   print('MAE is      {}'.format(test_score_mae ))\n",
    "   print('MSE is      {}'.format(test_score_mse ))\n",
    "   print('EVS is      {}'.format(test_score_evs ))\n",
    "   #print('ME is      {}'.format(test_score_me  ))\n",
    "   #print('MSLE is    {}'.format(test_score_msle))\n",
    "   print('R2 score is {}'.format(test_score_r2  ))\n",
    "   print()\n",
    "   print(\"Best parameters found for dev set:\")\n",
    "   print(gs.best_params_)\n",
    "   print()\n",
    "   \n",
    "   with open(data+\"/../\"+'output.log', 'w') as f:\n",
    "       #print(\"Training time: %.6f s\"   % runtime,      file=f)\n",
    "       #print(\"Prediction time: %.6f s\" % regr_predict, file=f)\n",
    "       print(\" \",                                      file=f)\n",
    "       print(\"The model performance for training set\", file=f)\n",
    "       print(\"--------------------------------------\", file=f)\n",
    "       print('MAE is      {}'.format(train_score_mae), file=f)\n",
    "       print('MSE is      {}'.format(train_score_mse), file=f)\n",
    "       print('EVS is      {}'.format(train_score_evs), file=f)\n",
    "       #print('ME is      {}'.format(train_score_me),  file=f)\n",
    "       #print('MSLE is    {}'.format(train_score_msle),file=f)\n",
    "       print('R2 score is {}'.format(train_score_r2),  file=f)\n",
    "       print(\" \",                                      file=f)\n",
    "       print(\"The model performance for testing set\",  file=f)\n",
    "       print(\"--------------------------------------\", file=f)\n",
    "       print('MAE is      {}'.format(test_score_mae),  file=f)\n",
    "       print('MSE is      {}'.format(test_score_mse),  file=f)\n",
    "       print('EVS is      {}'.format(test_score_evs),  file=f)\n",
    "       #print('ME is      {}'.format(test_score_me),   file=f)\n",
    "       #print('MSLE is    {}'.format(test_score_msle), file=f)\n",
    "       print('R2 score is {}'.format(test_score_r2),   file=f)\n",
    "       print(\" \",                                      file=f)\n",
    "       print(\"Best parameters found for dev set:\",     file=f)\n",
    "       print(gs.best_params_,                          file=f)\n",
    "\n",
    "\n",
    "def draw_plot(x_test_dim, y_test_dim, y_regr_dim, figure, data):\n",
    "   \n",
    "   plt.scatter(x_test_dim, y_test_dim[:,5], s=2, c='k', marker='o', label='Matlab')\n",
    "   plt.scatter(x_test_dim, y_regr_dim[:,5], s=2, c='purple', marker='+', label='DT, i=5')\n",
    "\n",
    "   plt.scatter(x_test_dim, y_test_dim[:,10], s=2, c='k', marker='o', label='Matlab')\n",
    "   plt.scatter(x_test_dim, y_regr_dim[:,10], s=2, c='r', marker='+', label='DT, i=10')\n",
    "\n",
    "   plt.scatter(x_test_dim, y_test_dim[:,15], s=2, c='k', marker='o', label='Matlab')\n",
    "   plt.scatter(x_test_dim, y_regr_dim[:,15], s=2, c='c', marker='+', label='DT, i=15')\n",
    "\n",
    "   plt.scatter(x_test_dim, y_test_dim[:,20], s=2, c='k', marker='o', label='Matlab')\n",
    "   plt.scatter(x_test_dim, y_regr_dim[:,20], s=2, c='g', marker='+', label='DT, i=20')\n",
    "\n",
    "   plt.scatter(x_test_dim, y_test_dim[:,25], s=2, c='k', marker='o', label='Matlab')\n",
    "   plt.scatter(x_test_dim, y_regr_dim[:,25], s=2, c='y', marker='+', label='DT, i=25')\n",
    "\n",
    "   plt.scatter(x_test_dim, y_test_dim[:,30], s=2, c='k', marker='o', label='Matlab')\n",
    "   plt.scatter(x_test_dim, y_regr_dim[:,30], s=2, c='b', marker='+', label='DT, i=30')\n",
    "\n",
    "   plt.scatter(x_test_dim, y_test_dim[:,35], s=2, c='k', marker='o', label='Matlab')\n",
    "   plt.scatter(x_test_dim, y_regr_dim[:,35], s=2, c='m', marker='+', label='DT, i=35')\n",
    "\n",
    "   #plt.ylabel(r'$\\eta$ [PaÂ·s]')\n",
    "   plt.xlabel('T [K] ')\n",
    "   plt.legend()\n",
    "   plt.tight_layout()\n",
    "   plt.savefig(figure+\"/regression_MO_\"+data+'.pdf')\n",
    "   #plt.show()\n",
    "   plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process:  \u001b[32mD\u001b[0m\n",
      "Algorithm:  \u001b[34mD\u001b[0m\n",
      "PWD:  \u001b[33m.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import argparse\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, make_scorer\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import estimators\n",
    "import utils\n",
    "\n",
    "from termcolor import colored\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "#    parser = argparse.ArgumentParser(description='reaction rates regression')\n",
    "#    parser.add_argument('-p', '--process', type=str,\n",
    "#                        choices=['DR', 'VT', 'VV', 'VV2', 'ZR'],\n",
    "#                        default='DR,VT,VV,VV2,ZR',\n",
    "#                        help='Comma-separated names of properties whose regression is performed')\n",
    "#\n",
    "#    parser.add_argument('-a', '--algorithm', type=str,\n",
    "#                        choices=['DT', 'RF', 'ET', 'GP', 'KN', 'SVM', 'KR', 'GB', 'HGB', 'MLP'],\n",
    "#                        default='DT',\n",
    "#                        help='regression algorithm')\n",
    "#\n",
    "#    args = parser.parse_args()\n",
    "\n",
    "    process   = \"DR\" #args.process.split(',')\n",
    "    directory = process[0]+'/data/processes'\n",
    "    path      = directory+\"/*.csv\"\n",
    "    print(\"Process: \", colored(process[0], 'green'))\n",
    "\n",
    "    algorithm = \"DT\" #args.algorithm.split(',')\n",
    "    print(\"Algorithm: \", colored(algorithm[0],'blue'))\n",
    "\n",
    "    parent_dir = \".\"\n",
    "    print(\"PWD: \", colored(parent_dir,'yellow'))\n",
    "\n",
    "    n_jobs = 2\n",
    "\n",
    "    for f in glob.glob(path):\n",
    "        #print(\"{bcolors.OKGREEN}f{bcolors.ENDC}\")\n",
    "        print(colored(f, 'red'))\n",
    "#FIXME: pd skip the first line \n",
    "        dataset_k = pd.read_csv(f, delimiter=\",\").to_numpy()\n",
    "        dataset_T = pd.read_csv(parent_dir+\"/\"+process[0]+\"/data/Temperatures.csv\").to_numpy()\n",
    "        \n",
    "        x = dataset_T.reshape(-1,1)\n",
    "        y = dataset_k\n",
    "\n",
    "        print(\"### Phase 1: PRE_PROCESSING ###\")\n",
    "        ########################################\n",
    "        data, dir, proc, model, scaler, figure = utils.mk_tree(f, parent_dir, process[0], algorithm[0])\n",
    "\n",
    "        # 3) train/test split dataset\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.75, test_size=0.25, random_state=69)\n",
    "\n",
    "        # 4) compute and save scaler\n",
    "        sc_x = StandardScaler()\n",
    "        sc_y = StandardScaler()\n",
    "        \n",
    "        sc_x.fit(x_train)\n",
    "        x_train = sc_x.transform(x_train)\n",
    "        x_test  = sc_x.transform(x_test)\n",
    "        \n",
    "        sc_y.fit(y_train)\n",
    "        y_train = sc_y.transform(y_train)\n",
    "        y_test  = sc_y.transform(y_test) \n",
    "\n",
    "        print('Training Features Shape:', x_train.shape)\n",
    "        print('Training Labels Shape:',   y_train.shape)\n",
    "        print('Testing Features Shape:',  x_test.shape)\n",
    "        print('Testing Labels Shape:',    y_test.shape)\n",
    "\n",
    "        dump(sc_x, open(scaler+\"/scaler_x_MO_\"+data+'.pkl', 'wb'))\n",
    "        dump(sc_y, open(scaler+\"/scaler_y_MO_\"+data+'.pkl', 'wb'))\n",
    "\n",
    "        if (algorithm[0] == 'DT'):\n",
    "            est, hyper_params = estimators.est_DT()\n",
    "\n",
    "        elif (algorithm[0] == 'ET'):\n",
    "            est, hyper_params = estimators.est_ET()\n",
    "\n",
    "        elif (algorithm[0] == 'SVM'):\n",
    "            est, hyper_params = estimators.est_SVM()\n",
    "\n",
    "        elif (algorithm[0] == 'KR'):\n",
    "            est, hyper_params = estimators.est_KR()\n",
    "\n",
    "        elif (algorithm[0] == 'KN'):\n",
    "            est, hyper_params = estimators.est_KN()\n",
    "\n",
    "        elif (algorithm[0] == 'MLP'):\n",
    "            est, hyper_params = estimators.est_MLP()\n",
    "\n",
    "        elif (algorithm[0] == 'GB'):\n",
    "            est, hyper_params = estimators.est_GB()\n",
    "\n",
    "        elif (algorithm[0] == 'HGB'):\n",
    "            est, hyper_params = estimators.est_HGB()\n",
    "\n",
    "        elif (algorithm[0] == 'RF'):\n",
    "            est, hyper_params = estimators.est_RF()\n",
    "\n",
    "        elif (algorithm[0] == 'GB'):\n",
    "            est, hyper_params = estimators.est_GB()\n",
    "\n",
    "        else:\n",
    "            print(\"Algorithm not implemented ...\")\n",
    "    \n",
    "\n",
    "        # Exhaustive search over specified parameter values for the estimator\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "        gs = GridSearchCV(est, cv=10, param_grid=hyper_params, verbose=2, n_jobs=n_jobs, scoring='r2',\n",
    "                          refit=True, pre_dispatch='n_jobs', error_score=np.nan, return_train_score=True)\n",
    "    \n",
    "\n",
    "        # Randomized search on hyper parameters\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV\n",
    "        # class sklearn.model_selection.RandomizedSearchCV(estimator, param_distributions, *, n_iter=10, scoring=None, n_jobs=None, refit=True, \n",
    "        #                                                  cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None, error_score=nan, \n",
    "        #                                                  return_train_score=False)\n",
    "        #gs = RandomizedSearchCV(est, cv=10, n_iter=10, param_distributions=hyper_params, verbose=2, n_jobs=n_jobs, scoring='r2',\n",
    "        #                        refit=True, pre_dispatch='n_jobs', error_score=np.nan, return_train_score=True)\n",
    "\n",
    "\n",
    "        utils.fit(x_train, y_train, gs)\n",
    "\n",
    "        results = pd.DataFrame(gs.cv_results_)\n",
    "        # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n",
    "        #compression_opts = dict(method='zip', archive_name='GridSearchCV_results.csv')\n",
    "        #results.to_csv('GridSearchCV_results.zip', index=False, compression=compression_opts)\n",
    "        results.to_csv(model+\"/../\"+\"GridSearchCV_results.csv\", index=False, sep='\\t', encoding='utf-8')\n",
    "     \n",
    "        #plt.figure(figsize=(12, 4))\n",
    "        #for score in ['mean_test_recall', 'mean_test_precision', 'mean_test_min_both']:\n",
    "        #    plt.plot([_[1] for _ in results['param_class_weight']], results[score], label=score)\n",
    "        #plt.legend();\n",
    "\n",
    "        #plt.figure(figsize=(12, 4))\n",
    "        #for score in ['mean_train_recall', 'mean_train_precision', 'mean_test_min_both']:\n",
    "        #    plt.scatter(x=[_[1] for _ in results['param_class_weight']], y=results[score.replace('test', 'train')], label=score)\n",
    "        #plt.legend();\n",
    "\n",
    "        # summarize results\n",
    "        print(\"Best: %f using %s\" % (gs.best_score_, gs.best_params_))\n",
    "        means  = gs.cv_results_['mean_test_score']\n",
    "        stds   = gs.cv_results_['std_test_score']\n",
    "        params = gs.cv_results_['params']\n",
    "        for mean, stdev, param in zip(means, stds, params):\n",
    "            print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "        y_regr = utils.predict(x_test, gs)\n",
    "        \n",
    "        utils.scores(sc_x, sc_y, x_train, y_train, x_test, y_test, model, gs)\n",
    "\n",
    "        x_test_dim = sc_x.inverse_transform(x_test)\n",
    "        y_test_dim = sc_y.inverse_transform(y_test)\n",
    "        y_regr_dim = sc_y.inverse_transform(y_regr)          \n",
    "\n",
    "        utils.draw_plot(x_test_dim, y_test_dim, y_regr_dim, figure, data)\n",
    "\n",
    "        # save the model to disk\n",
    "        dump(gs, model+\"/model_MO_\"+data+'.sav')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
