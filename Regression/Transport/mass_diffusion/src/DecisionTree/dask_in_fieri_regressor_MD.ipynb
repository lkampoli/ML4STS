{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lk/.local/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 43579 instead\n",
      "  http_address[\"port\"], self.http_server.port\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>inproc://10.0.0.119/26422/12</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.0.0.119:43579/status' target='_blank'>http://10.0.0.119:43579/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>4.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'inproc://10.0.0.119/26422/12' processes=2 threads=4, memory=4.00 GB>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://nbviewer.jupyter.org/github/PuneetGrov3r/MediumPosts/blob/master/Tackle/BigData-IncrementalLearningAndDask.ipynb#Method-2:-Using-Dask:\n",
    "# https://towardsdatascience.com/how-to-learn-from-bigdata-files-on-low-memory-incremental-learning-d377282d38ff#2d3d\n",
    "# https://towardsdatascience.com/speeding-up-your-algorithms-part-4-dask-7c6ed79994ef\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html\n",
    "\n",
    "# Import database\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "sys.path.insert(0, '../../../../Utilities/')\n",
    "\n",
    "from plotting import newfig, savefig\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import operator\n",
    "import itertools\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "n_jobs = 1\n",
    "trial  = 1\n",
    "\n",
    "import bz2\n",
    "from shutil import copyfileobj\n",
    "\n",
    "import dask\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "import dask.delayed as delay\n",
    "from dask.distributed import Client\n",
    "from dask_ml.model_selection import train_test_split, GridSearchCV\n",
    "client = Client(processes=False, threads_per_worker=2, n_workers=2, memory_limit='2GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dd.read_fwf(\"../../../Data/TCs_air5_MD.txt\", blocksize=10e6)\n",
    "data = pd.DataFrame(df).to_numpy()\n",
    "darr = da.from_array(data, chunks=(1000,1000))\n",
    "#df.npartitions\n",
    "darr.npartitions\n",
    "#df.visualize(size=\"7,5!\")\n",
    "\n",
    "#result = darr.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>P</th>\n",
       "      <th>x_N2</th>\n",
       "      <th>x_O2</th>\n",
       "      <th>x_NO</th>\n",
       "      <th>x_N</th>\n",
       "      <th>x_O</th>\n",
       "      <th>Eta</th>\n",
       "      <th>Zeta</th>\n",
       "      <th>Lam</th>\n",
       "      <th>...</th>\n",
       "      <th>[110]</th>\n",
       "      <th>[111]           [112]           [113]           [114]</th>\n",
       "      <th>[115]</th>\n",
       "      <th>[116]</th>\n",
       "      <th>[117]</th>\n",
       "      <th>[118]</th>\n",
       "      <th>[119]</th>\n",
       "      <th>[120]</th>\n",
       "      <th>[121]</th>\n",
       "      <th>Unnamed: 129</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.255530</td>\n",
       "      <td>0.065771</td>\n",
       "      <td>0.286183</td>\n",
       "      <td>0.261033</td>\n",
       "      <td>0.072110</td>\n",
       "      <td>0.041204</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.097647e-47</td>\n",
       "      <td>-2.114684e-48  -7.911150e-50  -3.191995e-51  -...</td>\n",
       "      <td>-3.293641e-55</td>\n",
       "      <td>-1.794685e-56</td>\n",
       "      <td>-1.053698e-57</td>\n",
       "      <td>-6.664651e-59</td>\n",
       "      <td>-4.540357e-60</td>\n",
       "      <td>-3.330974e-61</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.164873</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.129873</td>\n",
       "      <td>0.007793</td>\n",
       "      <td>0.678906</td>\n",
       "      <td>0.044471</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>1.927782e-46</td>\n",
       "      <td>6.685612e-48   2.501125e-49   1.009155e-50   4...</td>\n",
       "      <td>1.041291e-54</td>\n",
       "      <td>5.673928e-56</td>\n",
       "      <td>3.331285e-57</td>\n",
       "      <td>2.107042e-58</td>\n",
       "      <td>1.435442e-59</td>\n",
       "      <td>1.053094e-60</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.072908</td>\n",
       "      <td>0.311170</td>\n",
       "      <td>0.251743</td>\n",
       "      <td>0.291534</td>\n",
       "      <td>0.072646</td>\n",
       "      <td>0.041844</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.837734e-47</td>\n",
       "      <td>-6.373319e-49  -2.384295e-50  -9.620166e-52  -...</td>\n",
       "      <td>-9.926512e-56</td>\n",
       "      <td>-5.408892e-57</td>\n",
       "      <td>-3.175677e-58</td>\n",
       "      <td>-2.008620e-59</td>\n",
       "      <td>-1.368391e-60</td>\n",
       "      <td>-1.003902e-61</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.065731</td>\n",
       "      <td>0.103583</td>\n",
       "      <td>0.057109</td>\n",
       "      <td>0.231777</td>\n",
       "      <td>0.541800</td>\n",
       "      <td>0.044239</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.202527e-47</td>\n",
       "      <td>-4.170405e-49  -1.560171e-50  -6.294992e-52  -...</td>\n",
       "      <td>-6.495449e-56</td>\n",
       "      <td>-3.539328e-57</td>\n",
       "      <td>-2.078016e-58</td>\n",
       "      <td>-1.314347e-59</td>\n",
       "      <td>-8.954120e-61</td>\n",
       "      <td>-6.569073e-62</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.280106</td>\n",
       "      <td>0.103732</td>\n",
       "      <td>0.073565</td>\n",
       "      <td>0.480825</td>\n",
       "      <td>0.061772</td>\n",
       "      <td>0.041883</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>8.327097e-47</td>\n",
       "      <td>2.887864e-48   1.080366e-49   4.359068e-51   1...</td>\n",
       "      <td>4.497879e-55</td>\n",
       "      <td>2.450866e-56</td>\n",
       "      <td>1.438956e-57</td>\n",
       "      <td>9.101413e-59</td>\n",
       "      <td>6.200423e-60</td>\n",
       "      <td>4.548861e-61</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       T       P      x_N2      x_O2      x_NO       x_N       x_O       Eta  \\\n",
       "0  500.0  5000.0  0.255530  0.065771  0.286183  0.261033  0.072110  0.041204   \n",
       "1  500.0  5000.0  0.164873  0.018555  0.129873  0.007793  0.678906  0.044471   \n",
       "2  500.0  5000.0  0.072908  0.311170  0.251743  0.291534  0.072646  0.041844   \n",
       "3  500.0  5000.0  0.065731  0.103583  0.057109  0.231777  0.541800  0.044239   \n",
       "4  500.0  5000.0  0.280106  0.103732  0.073565  0.480825  0.061772  0.041883   \n",
       "\n",
       "       Zeta       Lam  ...         [110]  \\\n",
       "0  0.000029  0.000019  ... -6.097647e-47   \n",
       "1  0.000030  0.000019  ...  1.927782e-46   \n",
       "2  0.000029  0.000019  ... -1.837734e-47   \n",
       "3  0.000030  0.000019  ... -1.202527e-47   \n",
       "4  0.000029  0.000019  ...  8.327097e-47   \n",
       "\n",
       "   [111]           [112]           [113]           [114]         [115]  \\\n",
       "0  -2.114684e-48  -7.911150e-50  -3.191995e-51  -...     -3.293641e-55   \n",
       "1  6.685612e-48   2.501125e-49   1.009155e-50   4...      1.041291e-54   \n",
       "2  -6.373319e-49  -2.384295e-50  -9.620166e-52  -...     -9.926512e-56   \n",
       "3  -4.170405e-49  -1.560171e-50  -6.294992e-52  -...     -6.495449e-56   \n",
       "4  2.887864e-48   1.080366e-49   4.359068e-51   1...      4.497879e-55   \n",
       "\n",
       "          [116]         [117]         [118]         [119]         [120]  \\\n",
       "0 -1.794685e-56 -1.053698e-57 -6.664651e-59 -4.540357e-60 -3.330974e-61   \n",
       "1  5.673928e-56  3.331285e-57  2.107042e-58  1.435442e-59  1.053094e-60   \n",
       "2 -5.408892e-57 -3.175677e-58 -2.008620e-59 -1.368391e-60 -1.003902e-61   \n",
       "3 -3.539328e-57 -2.078016e-58 -1.314347e-59 -8.954120e-61 -6.569073e-62   \n",
       "4  2.450866e-56  1.438956e-57  9.101413e-59  6.200423e-60  4.548861e-61   \n",
       "\n",
       "      [121]  Unnamed: 129  \n",
       "0 -0.000078     -0.000083  \n",
       "1 -0.000063     -0.000067  \n",
       "2 -0.000077     -0.000082  \n",
       "3 -0.000059     -0.000062  \n",
       "4 -0.000068     -0.000071  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Series Structure:\n",
       "npartitions=10\n",
       "    float64\n",
       "        ...\n",
       "     ...   \n",
       "        ...\n",
       "        ...\n",
       "Name: x_N2, dtype: float64\n",
       "Dask Name: getitem, 20 tasks"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert string Series to dictionary Series\n",
    "#df['Dict'] = df['Dict'].apply(lambda x: json.loads(x.replace(\"'\", '\"')), meta=('Dict', 'f8'))\n",
    "#\n",
    "#dict_col_keys = {\n",
    "#    'Dict': ['MedInc', 'HouseAge']\n",
    "#}\n",
    "#\n",
    "#for dic_col in dict_col_keys:\n",
    "#  for key in dict_col_keys[dic_col]:\n",
    "#    df[f'{dic_col}.{key}'] = df[dic_col].to_bag().pluck(key).to_dataframe().iloc[:,0]\n",
    "#\n",
    "#df.columns\n",
    "#df.x_N2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4972 130\n",
      "4973 130\n",
      "4972 130\n",
      "4973 130\n",
      "4973 130\n",
      "4972 130\n",
      "4973 130\n",
      "4973 130\n",
      "4972 130\n",
      "2047 130\n"
     ]
    }
   ],
   "source": [
    "# Necessary for converting dataframe to array. Takes specified length from each block.\n",
    "lengths = []\n",
    "for part in df.partitions:\n",
    "  l = part.shape[0].compute()\n",
    "  lengths.append(l)\n",
    "  print(l, part.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<values, shape=(46800, 130), dtype=object, chunksize=(4973, 130), chunktype=numpy.ndarray>\n",
      "dask.array<values, shape=(46800, 130), dtype=object, chunksize=(4973, 130), chunktype=numpy.ndarray>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 48.67 MB </td> <td> 5.17 MB </td></tr>\n",
       "    <tr><th> Shape </th><td> (46800, 130) </td> <td> (4973, 130) </td></tr>\n",
       "    <tr><th> Count </th><td> 20 Tasks </td><td> 10 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> object </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"25\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" />\n",
       "  <line x1=\"0\" y1=\"38\" x2=\"25\" y2=\"38\" />\n",
       "  <line x1=\"0\" y1=\"51\" x2=\"25\" y2=\"51\" />\n",
       "  <line x1=\"0\" y1=\"63\" x2=\"25\" y2=\"63\" />\n",
       "  <line x1=\"0\" y1=\"76\" x2=\"25\" y2=\"76\" />\n",
       "  <line x1=\"0\" y1=\"89\" x2=\"25\" y2=\"89\" />\n",
       "  <line x1=\"0\" y1=\"102\" x2=\"25\" y2=\"102\" />\n",
       "  <line x1=\"0\" y1=\"114\" x2=\"25\" y2=\"114\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >130</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">46800</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<values, shape=(46800, 130), dtype=object, chunksize=(4973, 130), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.DataFrame(df)#.to_numpy()\n",
    "#print(data)\n",
    "X, y = df.to_dask_array(lengths=lengths) , df.to_dask_array(lengths=lengths)\n",
    "print(X)\n",
    "#X\n",
    "print(y)\n",
    "y\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import StandardScaler\n",
    "\n",
    "sc_x = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "\n",
    "Xo = dask.array.zeros((X.shape[0],1), chunks=(200000,1))\n",
    "\n",
    "#for i, col_ in enumerate(df.columns):\n",
    "#    y = sc_y.fit_transform(y.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "#    temp = sc_x.fit_transform(X[:,i].reshape(-1, 1))\n",
    "#    Xo = dask.array.concatenate([Xo, temp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-37ea3a969f8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#df[\"column\"] = rsc.fit_transform(df[\"column\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msc_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mx_test\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msc_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "from dask_ml.preprocessing import StandardScaler\n",
    "\n",
    "sc_x = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "\n",
    "#df[\"column\"] = rsc.fit_transform(df[\"column\"])\n",
    "\n",
    "sc_x.fit(x_train)\n",
    "x_train = sc_x.fit_transform(x_train)\n",
    "x_test  = sc_x.fit_transform(x_test)\n",
    "\n",
    "sc_y.fit(y_train)\n",
    "y_train = sc_y.transform(y_train)\n",
    "y_test  = sc_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.75, test_size=0.25, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs = GridSearchCV(estimator, param_grid, cv=10)\n",
    "#gs.fit(xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<values, shape=(46800, 129), dtype=object, chunksize=(4973, 129), chunktype=numpy.ndarray>\n",
      "dask.array<values, shape=(46800,), dtype=float64, chunksize=(4973,), chunktype=numpy.ndarray>\n"
     ]
    }
   ],
   "source": [
    "X, y = df.drop(['Eta'], axis=1).to_dask_array(lengths=lengths) , df['Eta'].to_dask_array(lengths=lengths)\n",
    "print(X)\n",
    "print(y)\n",
    "Xo = dask.array.zeros((X.shape[0],1), chunks=(200000,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "operand type(s) all returned NotImplemented from __array_ufunc__(<ufunc 'subtract'>, '__call__', dask.array<reshape, shape=(46800, 1), dtype=object, chunksize=(4973, 1), chunktype=numpy.ndarray>, array([10000.])): 'Array', 'ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7dbdbf151dd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobustScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mXo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mXo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lk/.local/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lk/.local/lib/python3.7/site-packages/dask_ml/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_centering\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_scaling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: operand type(s) all returned NotImplemented from __array_ufunc__(<ufunc 'subtract'>, '__call__', dask.array<reshape, shape=(46800, 1), dtype=object, chunksize=(4973, 1), chunktype=numpy.ndarray>, array([10000.])): 'Array', 'ndarray'"
     ]
    }
   ],
   "source": [
    "from dask_ml.preprocessing import RobustScaler\n",
    "for i, col_ in enumerate(df.columns):\n",
    "  if col_ == \"Target\":\n",
    "    rsc = RobustScaler()\n",
    "    y = rsc.fit_transform(y.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "  else:\n",
    "    rsc = RobustScaler()\n",
    "    temp = rsc.fit_transform(X[:,i].reshape(-1, 1))\n",
    "    Xo = dask.array.concatenate([Xo, temp], axis=1)\n",
    "\n",
    "Xo = Xo[:, 1:]\n",
    "\n",
    "#Xo[-5:].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lk/.local/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 46305 instead\n",
      "  http_address[\"port\"], self.http_server.port\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_fwf(\"../../../Data/TCs_air5.txt\")\n",
    "#print(df)\n",
    "#df = pd.read_fwf(\"../../../Data/TCs_air5.txt\").to_numpy()\n",
    "#print(df)\n",
    "#print(\"df done!\")\n",
    "\n",
    "# https://stackoverflow.com/questions/6884903/how-to-pickle-several-txt-files-into-one-pickle/6897610\n",
    "#file1=open('../../../Data/TCs_air5_MD.txt','r')\n",
    "#obj=[file1.read()]\n",
    "#pickle.dump(obj,open('result.i2','w'),2)\n",
    "\n",
    "#df = pd.read_fwf(\"../../../Data/TCs_air5_MD.txt\").to_numpy()\n",
    "#print(df)\n",
    "#print(\"df done!\")\n",
    "\n",
    "# https://stackoverflow.com/questions/9518705/big-file-compression-with-python\n",
    "#with open('../../../Data/TCs_air5_MD.txt', 'rb') as input:\n",
    "#with open('../../../Data/TCs_air5_MD_full.txt', 'rb') as input:\n",
    "#    with bz2.BZ2File('../../../Data/TCs_air5_MD_full.txt.bz2', 'wb', compresslevel=9) as output:\n",
    "#        copyfileobj(input, output)\n",
    "\n",
    "# https://stackoverflow.com/questions/20428355/appending-column-to-frame-of-hdf-file-in-pandas/20428786#20428786\n",
    "#store = pd.HDFStore('../../../Data/TCs_air5.h5',mode='w')\n",
    "#store = pd.HDFStore('../../../Data/TCs_air5_MD.h5',mode='w')\n",
    "#for chunk in pd.read_csv('../../../Data/TCs_air5.txt',chunksize=50000):\n",
    "#for chunk in pd.read_fwf('../../../Data/TCs_air5.txt',chunksize=50000):\n",
    "#for chunk in pd.read_fwf('../../../Data/TCs_air5_MD.txt',chunksize=100):\n",
    "#         store.append('df', chunk)\n",
    "#store.close()\n",
    "\n",
    "# https://medium.com/towards-artificial-intelligence/efficient-pandas-using-chunksize-for-large-data-sets-c66bf3037f93\n",
    "#data = pd.read_fwf('../../../Data/TCs_air5.txt')\n",
    "#df = pd.read_fwf('../../../Data/TCs_air5.txt',chunksize=100)\n",
    "#data = pd.read_fwf('../../../Data/TCs_air5_MD.txt',chunksize=100)\n",
    "#data = pd.read_fwf('../../../Data/TCs_air5_MD.txt') # too big!\n",
    "#df = pd.DataFrame(data)\n",
    "#print(df)\n",
    "#print(\"df done!\")\n",
    "\n",
    "# https://stackoverflow.com/questions/32358443/converting-a-dataset-into-an-hdf5-dataset\n",
    "#frame = pd.read_fwf('../../../Data/TCs_air5.txt')\n",
    "#type(frame)\n",
    "#pd.core.frame.DataFrame\n",
    "#hdf5store = pd.HDFStore('mydata.h5')\n",
    "#hdf5store['frame'] = frame\n",
    "##hdf5store\n",
    "#list(hdf5store.items())\n",
    "#hdf5store2 = pd.HDFStore('mydata.h5')\n",
    "#list(hdf5store2.items())\n",
    "#framecopy = hdf5store2['frame']\n",
    "##framecopy\n",
    "#framecopy = frame\n",
    "#hdf5store2.close()\n",
    "\n",
    "## https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas\n",
    "## create a store\n",
    "#store = pd.HDFStore('mystore.h5')\n",
    "#\n",
    "## this is the key to your storage:\n",
    "##    this maps your fields to a specific group, and defines\n",
    "##    what you want to have as data_columns.\n",
    "##    you might want to create a nice class wrapping this\n",
    "##    (as you will want to have this map and its inversion)\n",
    "#group_map = dict(\n",
    "#    A = dict(fields = ['field_1','field_2'], dc = ['field_1','field_5']),\n",
    "#    B = dict(fields = ['field_10'], dc = ['field_10']),\n",
    "#    REPORTING_ONLY = dict(fields = ['field_1000','field_1001'], dc = []),\n",
    "#)\n",
    "#\n",
    "#group_map_inverted = dict()\n",
    "#for g, v in group_map.items():\n",
    "#    group_map_inverted.update(dict([ (f,g) for f in v['fields'] ]))\n",
    "#\n",
    "#for f in files:\n",
    "#   # read in the file, additional options may be necessary here\n",
    "#   # the chunksize is not strictly necessary, you may be able to slurp each\n",
    "#   # file into memory in which case just eliminate this part of the loop\n",
    "#   # (you can also change chunksize if necessary)\n",
    "#   for chunk in pd.read_table(f, chunksize=1000):\n",
    "#       # we are going to append to each table by group\n",
    "#       # we are not going to create indexes at this time\n",
    "#       # but we *ARE* going to create (some) data_columns\n",
    "#\n",
    "#       # figure out the field groupings\n",
    "#       for g, v in group_map.items():\n",
    "#             # create the frame for this group\n",
    "#             frame = chunk.reindex(columns = v['fields'], copy = False)\n",
    "#\n",
    "#             # append it\n",
    "#             store.append(g, frame, index=False, data_columns = v['dc'])\n",
    "#\n",
    "#frame = store.select(group_that_I_want)\n",
    "## you can optionally specify:\n",
    "## columns = a list of the columns IN THAT GROUP (if you wanted to\n",
    "##     select only say 3 out of the 20 columns in this sub-table)\n",
    "## and a where clause if you want a subset of the rows\n",
    "#\n",
    "## do calculations on this frame\n",
    "#new_frame = cool_function_on_frame(frame)\n",
    "#\n",
    "## to 'add columns', create a new group (you probably want to\n",
    "## limit the columns in this new_group to be only NEW ones\n",
    "## (e.g. so you don't overlap from the other tables)\n",
    "## add this info to the group_map\n",
    "#store.append(new_group, new_frame.reindex(columns = new_columns_created, copy = False), data_columns = new_columns_created)\n",
    "\n",
    "# https://stackoverflow.com/questions/16149803/working-with-big-data-in-python-and-numpy-not-enough-ram-how-to-save-partial-r?lq=1\n",
    "# https://stackoverflow.com/questions/23872942/sklearn-and-large-datasets\n",
    "#a = np.memmap('test.mymemmap', dtype='float32', mode='w+', shape=(200000,1000))\n",
    "## here you will see a 762MB file created in your working directory\n",
    "#del a\n",
    "#b = np.memmap('test.mymemmap', dtype='float32', mode='r+', shape=(200000,1000))\n",
    "#b = np.memmap('test.mymemmap', dtype='float32', mode='r+', shape=(2,1000))\n",
    "#b[1,5] = 123456.\n",
    "#print(a[1,5])\n",
    "##123456.0\n",
    "#b = numpy.memmap('test.mymemmap', dtype='float32', mode='r+', shape=(2,1000), offset=150000*1000*32/8)\n",
    "#b[1,2] = 999999.\n",
    "#print(a[150001,2])\n",
    "##999999.0\n",
    "\n",
    "# https://towardsdatascience.com/how-to-learn-from-bigdata-files-on-low-memory-incremental-learning-d377282d38ff\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\"\n",
    "    iterate through all the columns of a dataframe and\n",
    "    modify the data type to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(('Memory usage of dataframe is {:.2f}' 'MB').format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            #df[col] = df[col].astype('category')\n",
    "            end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(('Memory usage after optimization is: {:.2f}' 'MB').format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "#reduce_mem_usage(df)\n",
    "#print(df)\n",
    "\n",
    "#incremental_dataframe = pd.read_fwf(\"../../../Data/TCs_air5.txt\", chunksize=100) # Number of lines to read.\n",
    "#print(incremental_dataframe)\n",
    "#idf = pd.DataFrame(incremental_dataframe).to_numpy()\n",
    "#print(idf)\n",
    "#\n",
    "#for df in incremental_dataframe:\n",
    "#    print(0)\n",
    "\n",
    "# http://pandas-docs.github.io/pandas-docs-travis/user_guide/io.html#io-perf\n",
    "# https://stackoverflow.com/questions/16628329/hdf5-concurrency-compression-i-o-performance\n",
    "# https://stackoverflow.com/questions/38515433/pickle-dump-pandas-dataframe\n",
    "def test_sql_write(df):\n",
    "    if os.path.exists('test.sql'):\n",
    "        os.remove('test.sql')\n",
    "    sql_db = sqlite3.connect('test.sql')\n",
    "    df.to_sql(name='../../../Data/test_table', con=sql_db)\n",
    "    sql_db.close()\n",
    "\n",
    "def test_sql_read():\n",
    "    sql_db = sqlite3.connect('../../../Data/test.sql')\n",
    "    pd.read_sql_query(\"select * from test_table\", sql_db)\n",
    "    sql_db.close()\n",
    "\n",
    "def test_hdf_fixed_write(df):\n",
    "    df.to_hdf('../../../Data/test_fixed.hdf', 'test', mode='w')\n",
    "\n",
    "def test_hdf_fixed_read():\n",
    "    pd.read_hdf('../../../Data/test_fixed.hdf', 'test')\n",
    "\n",
    "def test_hdf_fixed_write_compress(df):\n",
    "    df.to_hdf('../../../Data/test_fixed_compress.hdf', 'test', mode='w', complib='blosc')\n",
    "\n",
    "def test_hdf_fixed_read_compress():\n",
    "    pd.read_hdf('../../../Data/test_fixed_compress.hdf', 'test')\n",
    "\n",
    "def test_hdf_table_write(df):\n",
    "    df.to_hdf('../../../Data/test_table.hdf', 'test', mode='w', format='table')\n",
    "\n",
    "def test_hdf_table_read():\n",
    "    pd.read_hdf('../../../Data/test_table.hdf', 'test')\n",
    "\n",
    "def test_hdf_table_write_compress(df):\n",
    "    df.to_hdf('../../../Data/test_table_compress.hdf', 'test', mode='w',\n",
    "              complib='blosc', format='table')\n",
    "\n",
    "def test_hdf_table_read_compress():\n",
    "    pd.read_hdf('../../../Data/test_table_compress.hdf', 'test')\n",
    "\n",
    "def test_csv_write(df):\n",
    "    df.to_csv('../../../Data/test.csv', mode='w')\n",
    "\n",
    "def test_csv_read():\n",
    "    pd.read_csv('../../../Data/test.csv', index_col=0)\n",
    "\n",
    "def test_feather_write(df):\n",
    "    df.to_feather('../../../Data/test.feather')\n",
    "\n",
    "def test_feather_read():\n",
    "    pd.read_feather('../../../Data/test.feather')\n",
    "\n",
    "def test_pickle_write(df):\n",
    "    df.to_pickle('../../../Data/test.pkl')\n",
    "\n",
    "def test_pickle_read():\n",
    "    pd.read_pickle('../../../Data/test.pkl')\n",
    "\n",
    "def test_pickle_write_compress(df):\n",
    "    df.to_pickle('../../../Data/test.pkl.compress', compression='xz')\n",
    "\n",
    "def test_pickle_read_compress():\n",
    "    pd.read_pickle('../../../Data/test.pkl.compress', compression='xz')\n",
    "\n",
    "def test_parquet_write(df):\n",
    "    df.to_parquet('../../../Data/test.parquet')\n",
    "\n",
    "def test_parquet_read():\n",
    "    pd.read_parquet('../../../Data/test.parquet')\n",
    "\n",
    "# Write\n",
    "\n",
    "#data = pd.read_fwf('../../../Data/TCs_air5.txt')\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "#start_time = time.time()\n",
    "#test_hdf_fixed_write(df)\n",
    "#print(\"test_hdf_fixed_write --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_hdf_fixed_write(df)\n",
    "#print(\"test_hdf_fixed_write --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "##start_time = time.time()\n",
    "##test_sql_write(df)\n",
    "##print(\"test_sql_write --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_hdf_fixed_write(df)\n",
    "#print(\"test_hdf_fixed_write --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_hdf_fixed_write_compress(df)\n",
    "#print(\"test_hdf_fixed_write_compress --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_hdf_table_write(df)\n",
    "#print(\"test_hdf_table_write --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_hdf_table_write_compress(df)\n",
    "#print(\"test_hdf_table_write_compress --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_csv_write(df)\n",
    "#print(\"test_csv_write --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_feather_write(df)\n",
    "#print(\"test_feather_write --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_pickle_write(df)\n",
    "#print(\"test_pickle_write --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_pickle_write_compress(df)\n",
    "#print(\"test_pickle_write_compress --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_parquet_write(df)\n",
    "#print(\"test_parquet_write --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "# Read\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_sql_read()\n",
    "#print(\"test_sql_read --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_hdf_fixed_read()\n",
    "#print(\"test_hdf_fixed_read --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_hdf_fixed_read_compress()\n",
    "#print(\"test_hdf_fixed_read_compress --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_hdf_table_read()\n",
    "#print(\"test_hdf_table_read --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_hdf_table_read_compress()\n",
    "#print(\"test_hdf_table_read_compress --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_csv_read()\n",
    "#print(\"test_csv_read --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_feather_read()\n",
    "#print(\"test_feather_read --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_pickle_read()\n",
    "#print(\"test_pickle_read --- %s seconds ---\" % (time.time() - start_time))\n",
    "#\n",
    "#start_time = time.time()\n",
    "#test_pickle_read_compress()\n",
    "#print(\"test_pickle_read_compress --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "## The data is then split into training and test data\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.75, test_size=0.25, random_state=69)\n",
    "#\n",
    "#sc_x = StandardScaler()\n",
    "#sc_y = StandardScaler()\n",
    "#\n",
    "#sc_x.fit(x_train)\n",
    "#x_train = sc_x.fit_transform(x_train)\n",
    "#x_test  = sc_x.fit_transform(x_test)\n",
    "#\n",
    "#sc_y.fit(y_train)\n",
    "#y_train = sc_y.transform(y_train)\n",
    "#y_test  = sc_y.transform(y_test)\n",
    "#\n",
    "#dump(sc_x, open('../scaler/scaler_x_MD.pkl', 'wb'))\n",
    "#dump(sc_y, open('../scaler/scaler_y_MD.pkl', 'wb'))\n",
    "#\n",
    "#print('Training Features Shape:', x_train.shape)\n",
    "#print('Training Labels Shape:', y_train.shape)\n",
    "#print('Testing Features Shape:', x_test.shape)\n",
    "#print('Testing Labels Shape:', y_test.shape)\n",
    "#\n",
    "#regr = DecisionTreeRegressor(criterion='mse',\n",
    "#                             splitter='best',\n",
    "#                             max_features='auto',\n",
    "#                             random_state=69)\n",
    "#\n",
    "#regr = MultiOutputRegressor(estimator=regr)\n",
    "#\n",
    "#t0 = time.time()\n",
    "#with parallel_backend(\"dask\"):\n",
    "#    regr.fit(x_train, y_train)\n",
    "#regr_fit = time.time() - t0\n",
    "#print(\"Complexity and bandwidth selected and model fitted in %.6f s\" % regr_fit)\n",
    "#\n",
    "#t0 = time.time()\n",
    "#y_regr = regr.predict(x_test)\n",
    "#regr_predict = time.time() - t0\n",
    "#print(\"Prediction for %d inputs in %.6f s\" % (x_test.shape[0], regr_predict))\n",
    "#\n",
    "#x_test_dim = sc_x.inverse_transform(x_test)\n",
    "#y_test_dim = sc_y.inverse_transform(y_test)\n",
    "#y_regr_dim = sc_y.inverse_transform(y_regr)\n",
    "#\n",
    "#plt.scatter(x_test_dim[:,0], y_test_dim[:,0], s=5, c='k', marker='o', label='KAPPA')\n",
    "#plt.scatter(x_test_dim[:,0], y_regr_dim[:,0], s=5, c='r', marker='d', label='k-Nearest Neighbour')\n",
    "#plt.scatter(x_test_dim[:,0], y_test_dim[:,1], s=5, c='k', marker='o', label='KAPPA')\n",
    "#plt.scatter(x_test_dim[:,0], y_regr_dim[:,1], s=5, c='r', marker='d', label='k-Nearest Neighbour')\n",
    "##plt.scatter(x_test_dim[:,0], y_test_dim[:,2], s=5, c='k', marker='o', label='KAPPA')\n",
    "##plt.scatter(x_test_dim[:,0], y_regr_dim[:,2], s=5, c='r', marker='d', label='k-Nearest Neighbour')\n",
    "##plt.title('Shear viscosity regression with kNN')\n",
    "##plt.ylabel(r'$\\eta$ [Pa·s]')\n",
    "#plt.ylabel(' ')\n",
    "#plt.xlabel('T [K] ')\n",
    "#plt.legend()\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"../pdf/regression_MD.pdf\", dpi=150, crop='false')\n",
    "#plt.show()\n",
    "#\n",
    "## save the model to disk\n",
    "#dump(regr, '../model/model_MD.sav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
