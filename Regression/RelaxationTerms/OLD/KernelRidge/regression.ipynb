{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0, '../../Utilities/')\n",
    "from plotting import newfig, savefig\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import operator\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn import kernel_ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_jobs = 1\n",
    "trial  = 1\n",
    "\n",
    "#dataset=np.loadtxt(\"../data/datarelax.txt\")\n",
    "dataset=np.loadtxt(\"../data/datasetDR.txt\")\n",
    "#dataset=np.loadtxt(\"../data/datasetVT.txt\")\n",
    "#dataset=np.loadtxt(\"../data/datasetVV.txt\")\n",
    "x=dataset[:,2:3]   # 0: x [m], 1: t [s], 2: T [K]\n",
    "y=dataset[:,9:10]  # Rci (relaxation source terms)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.75, test_size=0.25, random_state=69, shuffle=True)\n",
    "\n",
    "# https://stackoverflow.com/questions/43675665/when-scale-the-data-why-the-train-dataset-use-fit-and-transform-but-the-te\n",
    "# https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/\n",
    "\n",
    "#sc_x = MinMaxScaler(feature_range=(0, 1))\n",
    "sc_x = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "\n",
    "# fit scaler\n",
    "sc_x.fit(x_train)\n",
    "# transform training datasetx\n",
    "x_train = sc_x.transform(x_train)\n",
    "# transform test dataset\n",
    "x_test = sc_x.transform(x_test)\n",
    "\n",
    "#y_train = y_train.reshape(len(y_train), 1)\n",
    "#y_test = y_test.reshape(len(y_train), 1)\n",
    "\n",
    "# fit scaler on training dataset\n",
    "sc_y.fit(y_train)\n",
    "# transform training dataset\n",
    "y_train = sc_y.transform(y_train)\n",
    "# transform test dataset\n",
    "y_test = sc_y.transform(y_test)\n",
    "\n",
    "print('Training Features Shape:', x_train.shape)\n",
    "print('Training Labels Shape:', y_train.shape)\n",
    "print('Testing Features Shape:', x_test.shape)\n",
    "print('Testing Labels Shape:', y_test.shape)\n",
    "\n",
    "hyper_params = [{'kernel': ('poly', 'rbf',),\n",
    "                 'alpha': (1e-1, 0.0, 0.1,),\n",
    "                 'gamma': (0.1, 1, 10, 100,),}]\n",
    "\n",
    "est=kernel_ridge.KernelRidge()\n",
    "gs = GridSearchCV(est, cv=10, param_grid=hyper_params, verbose=2, n_jobs=n_jobs, scoring='r2')\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(x_train, y_train.ravel())\n",
    "runtime = time.time() - t0\n",
    "print(\"KR complexity and bandwidth selected and model fitted in %.6f s\" % runtime)\n",
    "\n",
    "train_score_mse = mean_squared_error(      sc_y.inverse_transform(y_train), sc_y.inverse_transform(gs.predict(x_train)))\n",
    "train_score_mae = mean_absolute_error(     sc_y.inverse_transform(y_train), sc_y.inverse_transform(gs.predict(x_train)))\n",
    "train_score_evs = explained_variance_score(sc_y.inverse_transform(y_train), sc_y.inverse_transform(gs.predict(x_train)))\n",
    "train_score_me  = max_error(               sc_y.inverse_transform(y_train), sc_y.inverse_transform(gs.predict(x_train)))\n",
    "\n",
    "test_score_mse  = mean_squared_error(      sc_y.inverse_transform(y_test),  sc_y.inverse_transform(gs.predict(x_test)))\n",
    "test_score_mae  = mean_absolute_error(     sc_y.inverse_transform(y_test),  sc_y.inverse_transform(gs.predict(x_test)))\n",
    "test_score_evs  = explained_variance_score(sc_y.inverse_transform(y_test),  sc_y.inverse_transform(gs.predict(x_test)))\n",
    "test_score_me   = max_error(               sc_y.inverse_transform(y_test),  sc_y.inverse_transform(gs.predict(x_test)))\n",
    "test_score_r2   = r2_score(                sc_y.inverse_transform(y_test),  sc_y.inverse_transform(gs.predict(x_test)))\n",
    "\n",
    "print(\"The model performance for testing set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('MAE is {}'.format(test_score_mae))\n",
    "print('MSE is {}'.format(test_score_mse))\n",
    "print('EVS is {}'.format(test_score_evs))\n",
    "print('ME is {}'.format(test_score_me))\n",
    "print('R2 score is {}'.format(test_score_r2))\n",
    "\n",
    "sorted_grid_params = sorted(gs.best_params_.items(), key=operator.itemgetter(0))\n",
    "\n",
    "out_text = '\\t'.join(['regression',\n",
    "                      str(trial),\n",
    "                      str(sorted_grid_params).replace('\\n',','),\n",
    "                      str(train_score_mse),\n",
    "                      str(train_score_mae),\n",
    "                      str(train_score_evs),\n",
    "                      str(train_score_me),\n",
    "                      str(test_score_mse),\n",
    "                      str(test_score_mae),\n",
    "                      str(test_score_evs),\n",
    "                      str(test_score_me),\n",
    "                      str(runtime)])\n",
    "print(out_text)\n",
    "sys.stdout.flush()\n",
    "\n",
    "best_kernel = gs.best_params_['kernel']\n",
    "best_alpha  = gs.best_params_['alpha']\n",
    "best_gamma  = gs.best_params_['gamma']\n",
    "\n",
    "outF = open(\"output.txt\", \"w\")\n",
    "print('best_kernel = ', best_kernel, file=outF)\n",
    "print('best_alpha = ', best_alpha, file=outF)\n",
    "print('best_gamma = ', best_gamma, file=outF)\n",
    "print('R2 score is {}'.format(test_score_r2))\n",
    "outF.close()\n",
    "\n",
    "regr = KernelRidge(kernel=best_kernel, gamma=best_gamma, alpha=best_alpha)\n",
    "\n",
    "t0 = time.time()\n",
    "regr.fit(x_train, y_train.ravel())\n",
    "regr_fit = time.time() - t0\n",
    "print(\"Complexity and bandwidth selected and model fitted in %.6f s\" % regr_fit)\n",
    "\n",
    "t0 = time.time()\n",
    "y_regr = regr.predict(x_test)\n",
    "regr_predict = time.time() - t0\n",
    "print(\"Prediction for %d inputs in %.6f s\" % (x_test.shape[0], regr_predict))\n",
    "\n",
    "# open a file to append\n",
    "outF = open(\"output.txt\", \"a\")\n",
    "print(\"KR complexity and bandwidth selected and model fitted in %.6f s\" % regr_fit, file=outF)\n",
    "print(\"KR prediction for %d inputs in %.6f s\" % (x_test.shape[0], regr_predict),file=outF)\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, y_regr), file=outF)\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, y_regr), file=outF)\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_regr)), file=outF)\n",
    "outF.close()\n",
    "\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, y_regr))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, y_regr))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_regr)))\n",
    "\n",
    "\n",
    "x_test_dim = sc_x.inverse_transform(x_test)\n",
    "y_test_dim = sc_y.inverse_transform(y_test)\n",
    "y_regr_dim = sc_y.inverse_transform(y_regr)\n",
    "\n",
    "plt.scatter(x_test_dim, y_test_dim, s=2, c='k', marker='o', label='Matlab')\n",
    "plt.scatter(x_test_dim, y_regr_dim, s=2, c='r', marker='+', label='KernelRidge')\n",
    "#plt.title('Relaxation term $R_{ci}$ regression')\n",
    "plt.ylabel('$R_{ci}$ $[J/m^3/s]$')\n",
    "plt.xlabel('T [K] ')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"regression_KR.eps\", dpi=150, crop='false')\n",
    "#plt.savefig(\"regression_KR.pdf\", dpi=150, crop='false')\n",
    "plt.show()\n",
    "\n",
    "## Look at the results\n",
    "#gs_ind = gs.best_estimator_.support_\n",
    "##plt.scatter(x[gs_ind], y[gs_ind], c='r', s=50, label='KR',   zorder=2, edgecolors=(0, 0, 0))\n",
    "#plt.scatter(x_test_dim, y_test_dim, c='k',       label='data', zorder=1, edgecolors=(0, 0, 0))\n",
    "#plt.plot(x_test_dim, y_regr_dim, c='r', label='KR (fit: %.6fs, predict: %.6fs)' % (regr_fit, regr_predict))\n",
    "##plt.plot(X_plot, y_kr, c='g', label='KRR (fit: %.3fs, predict: %.3fs)' % (regr_fit, regr_predict))\n",
    "#plt.xlabel('data')\n",
    "#plt.ylabel('target')\n",
    "##plt.title('SVR versus Kernel Ridge')\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm                                                                                                             \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor                                                                                   \n",
    "from sklearn.neighbors import RadiusNeighborsRegressor                                                                              \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Visualize learning curves\n",
    "plt.figure()\n",
    "\n",
    "svr = SVR(kernel='rbf', epsilon=0.01, C=100., gamma='scale', coef0=0.0)\n",
    "kr = KernelRidge(kernel='rbf', gamma=100., alpha=0.0)\n",
    "dt = DecisionTreeRegressor(criterion='mse', splitter='best', max_features='auto', random_state=69)\n",
    "et = ExtraTreesRegressor(n_estimators=2000, min_weight_fraction_leaf=0.0, max_depth=None, \n",
    "                         max_leaf_nodes=300, min_samples_split=10, min_samples_leaf=1)\n",
    "gb = GradientBoostingRegressor(n_estimators=100, min_weight_fraction_leaf=0.0,\n",
    "                               max_features='auto', warm_start=False, criterion='mae', max_depth=None,\n",
    "                               loss='ls', min_samples_split=2, min_samples_leaf=5)\n",
    "knn = KNeighborsRegressor(n_neighbors=9, algorithm='ball_tree', leaf_size=1, weights='distance', p=1)\n",
    "                        \n",
    "train_sizes, train_scores_svr, test_scores_svr, fit_times_svr, score_times_svr = \\\n",
    "    learning_curve(svr, x_train, y_train.ravel(), train_sizes=np.linspace(0.1, 1, 10),\n",
    "                   scoring=\"neg_mean_squared_error\", cv=10, return_times=True)\n",
    "\n",
    "train_sizes, train_scores_kr, test_scores_kr, fit_times_kr, score_times_kr = \\\n",
    "    learning_curve(kr, x_train, y_train.ravel(), train_sizes=np.linspace(0.1, 1, 10),\n",
    "                   scoring=\"neg_mean_squared_error\", cv=10, return_times=True)\n",
    "\n",
    "train_sizes, train_scores_dt, test_scores_dt, fit_times_dt, score_times_dt = \\\n",
    "    learning_curve(dt, x_train, y_train.ravel(), train_sizes=np.linspace(0.1, 1, 10),\n",
    "                   scoring=\"neg_mean_squared_error\", cv=10, return_times=True)\n",
    "\n",
    "train_sizes, train_scores_et, test_scores_et, fit_times_et, score_times_et = \\\n",
    "    learning_curve(et, x_train, y_train.ravel(), train_sizes=np.linspace(0.1, 1, 10),\n",
    "                   scoring=\"neg_mean_squared_error\", cv=10, return_times=True)\n",
    "\n",
    "train_sizes, train_scores_gb, test_scores_gb, fit_times_gb, score_times_gb = \\\n",
    "    learning_curve(gb, x_train, y_train.ravel(), train_sizes=np.linspace(0.1, 1, 10),\n",
    "                   scoring=\"neg_mean_squared_error\", cv=10, return_times=True)\n",
    "\n",
    "train_sizes, train_scores_knn, test_scores_knn, fit_times_knn, score_times_knn = \\\n",
    "    learning_curve(knn, x_train, y_train.ravel(), train_sizes=np.linspace(0.1, 1, 10),\n",
    "                   scoring=\"neg_mean_squared_error\", cv=10, return_times=True)\n",
    "\n",
    "plt.plot(train_sizes, -test_scores_svr.mean(1), 'o-', color=\"b\", label=\"SVR\")\n",
    "plt.plot(train_sizes, -test_scores_kr.mean(1), 'o-', color=\"g\", label=\"KR\")\n",
    "plt.plot(train_sizes, -test_scores_dt.mean(1), 'o-', color=\"k\", label=\"DT\")\n",
    "plt.plot(train_sizes, -test_scores_et.mean(1), 'o-', color=\"y\", label=\"ET\")\n",
    "plt.plot(train_sizes, -test_scores_gb.mean(1), 'o-', color=\"r\", label=\"GB\")\n",
    "plt.plot(train_sizes, -test_scores_knn.mean(1), 'o-', color=\"c\", label=\"kNN\")\n",
    "plt.xlabel(\"Train size\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title('Learning curves')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize training and prediction time\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_kernel_ridge_regression.html#sphx-glr-auto-examples-miscellaneous-plot-kernel-ridge-regression-py\n",
    "from sklearn import svm                                                                                                             \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Generate sample data\n",
    "sizes = np.logspace(1, 4, 7).astype(np.int)\n",
    "for name, estimator in {\"KR\":  KernelRidge(kernel='rbf', gamma=100., alpha=0.0),\n",
    "                        \"SVR\": SVR(kernel='rbf', epsilon=0.01, C=100., gamma='scale', coef0=0.0),\n",
    "                        \"DT\":  DecisionTreeRegressor(criterion='mse', splitter='best', max_features='auto', random_state=69),\n",
    "                        \"ET\":  ExtraTreesRegressor(n_estimators=2000, min_weight_fraction_leaf=0.0, max_depth=None, \n",
    "                                                   max_leaf_nodes=300, min_samples_split=10, min_samples_leaf=1),\n",
    "                        \"GB\":  GradientBoostingRegressor(n_estimators=100, min_weight_fraction_leaf=0.0,\n",
    "                                                         max_features='auto', warm_start=False,\n",
    "                                                         criterion='mae', max_depth=None,\n",
    "                                                         loss='ls', min_samples_split=2,\n",
    "                                                         min_samples_leaf=5),\n",
    "                        \"kNN\": KNeighborsRegressor(n_neighbors=9, algorithm='ball_tree', leaf_size=1, weights='distance', p=1)\n",
    "                        \n",
    "                       }.items():\n",
    "    train_time = []\n",
    "    test_time = []\n",
    "    for train_test_size in sizes:\n",
    "        t0 = time.time()\n",
    "        regr.fit(x_train[:train_test_size,:], y_train[:train_test_size].ravel())\n",
    "        train_time.append(time.time() - t0)\n",
    "\n",
    "        t0 = time.time()\n",
    "        regr.predict(x_test)\n",
    "        test_time.append(time.time() - t0)\n",
    "\n",
    "    plt.plot(sizes, train_time, 'o-', color=\"r\" if name == \"SVR\" else \"k\", label=\"%s (train)\" % name)\n",
    "    plt.plot(sizes, test_time, 'o--', color=\"r\" if name == \"SVR\" else \"k\", label=\"%s (train)\" % name)\n",
    "    plt.plot(sizes, train_time, 'o-', color=\"g\" if name == \"KR\"  else \"k\", label=\"%s (test)\" % name)\n",
    "    plt.plot(sizes, test_time, 'o--', color=\"g\" if name == \"KR\"  else \"k\", label=\"%s (test)\" % name)\n",
    "    plt.plot(sizes, train_time, 'o-', color=\"b\" if name == \"DT\"  else \"k\", label=\"%s (test)\" % name)\n",
    "    plt.plot(sizes, test_time, 'o--', color=\"b\" if name == \"DT\"  else \"k\", label=\"%s (test)\" % name)\n",
    "    plt.plot(sizes, train_time, 'o-', color=\"m\" if name == \"ET\"  else \"k\", label=\"%s (test)\" % name)\n",
    "    plt.plot(sizes, test_time, 'o--', color=\"m\" if name == \"ET\"  else \"k\", label=\"%s (test)\" % name)\n",
    "    plt.plot(sizes, train_time, 'o-', color=\"y\" if name == \"GB\"  else \"k\", label=\"%s (test)\" % name)\n",
    "    plt.plot(sizes, test_time, 'o--', color=\"y\" if name == \"GB\"  else \"k\", label=\"%s (test)\" % name)\n",
    "    plt.plot(sizes, train_time, 'o-', color=\"c\" if name == \"kNN\"  else \"k\", label=\"%s (test)\" % name)\n",
    "    plt.plot(sizes, test_time, 'o--', color=\"c\" if name == \"kNN\"  else \"k\", label=\"%s (test)\" % name)\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Train size\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title('Execution Time')\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "# save the model to disk\n",
    "#dump(gs, 'model_KR.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
