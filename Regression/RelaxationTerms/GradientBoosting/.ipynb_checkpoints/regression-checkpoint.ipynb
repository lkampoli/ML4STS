{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (192, 1)\n",
      "Training Labels Shape: (192, 1)\n",
      "Testing Features Shape: (65, 1)\n",
      "Testing Labels Shape: (65, 1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0, '../../Utilities/')\n",
    "from plotting import newfig, savefig\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import operator\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, cross_val_score\n",
    "#from sklearn import kernel_ridge\n",
    "#from sklearn.kernel_ridge import KernelRidge\n",
    "#from sklearn.neighbors import KNeighborsRegressor\n",
    "#from sklearn.neighbors import RadiusNeighborsRegressor\n",
    "#from sklearn import neighbors\n",
    "#from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import ensemble\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.ensemble import ExtraTreesRegressor\n",
    "#from sklearn import svm\n",
    "#from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "n_jobs = 1\n",
    "trial  = 1\n",
    "\n",
    "dataset=np.loadtxt(\"../data/datarelax.txt\")\n",
    "\n",
    "# ... only for plotting\n",
    "#dataset=np.loadtxt(\"../data/datarelax.txt\")\n",
    "#x=dataset[:,0:1]   # Temperatures\n",
    "#y=dataset[:,1:50]  # Rci (relaxation source terms)\n",
    "\n",
    "#for i in range (2,48):\n",
    "#    plt.scatter(dataset[:,0:1], dataset[:,i], s=0.5, label=i)\n",
    "\n",
    "#plt.title('$R_{ci}$ for $N_2/N$')\n",
    "#plt.xlabel('T [K]')\n",
    "#plt.ylabel('$R_{ci}$ $[J/m^3/s]$')\n",
    "##plt.legend()\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"relaxation_source_terms.pdf\")\n",
    "#plt.show()\n",
    "\n",
    "# Here, I learn one specific level of R_ci spanning all temperatures\n",
    "x=dataset[:,0:1]   # Temperatures\n",
    "y=dataset[:,9:10]  # Rci (relaxation source terms)\n",
    "\n",
    "# Here, I fix the temperature and learn all levels of R_ci\n",
    "#x=dataset[150,0:1]   # Temperatures\n",
    "#y=dataset[150,1:50]  # Rci (relaxation source terms)\n",
    "\n",
    "# TODO: Here, I want to learn all T and all Rci alltogether\n",
    "#x=dataset[:,0:1]   # Temperatures\n",
    "#y=dataset[:,1:50]  # Rci (relaxation source terms)\n",
    "\n",
    "# 2D Plot\n",
    "#plt.scatter(x, y, s=0.5)\n",
    "#plt.title('$R_{ci}$ for $N_2/N$ and i = 10')\n",
    "#plt.xlabel('T [K]')\n",
    "#plt.ylabel('$R_{ci}$ $[J/m^3/s]$')\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"relaxation_source_terms.pdf\")\n",
    "#plt.show()\n",
    "\n",
    "y=np.reshape(y, (-1,1))\n",
    "sc_x = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X = sc_x.fit_transform(x)\n",
    "Y = sc_y.fit_transform(y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "print('Training Features Shape:', x_train.shape)\n",
    "print('Training Labels Shape:', y_train.shape)\n",
    "print('Testing Features Shape:', x_test.shape)\n",
    "print('Testing Labels Shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KernelRidge\n",
    "#hyper_params = [{'kernel': ('poly','rbf',), 'alpha': (1e-4,1e-2,0.1,1,10,), 'gamma': (0.01,0.1,1,10,100,),}]\n",
    "\n",
    "# k-kearest neighbor\n",
    "#hyper_params = [{'algorithm': ('ball_tree', 'kd_tree', 'brute',), 'n_neighbors': (1,2,3,4,5,6,7,8,9,10,),\n",
    "#                 'leaf_size': (1, 10, 20, 30, 100,), 'weights': ('uniform', 'distance',), 'p': (1,2,),}]\n",
    "\n",
    "# Random Forest\n",
    "#hyper_params = [{'n_estimators': (10, 100, 1000),\n",
    "#                 'min_weight_fraction_leaf': (0.0, 0.25, 0.5),\n",
    "#                 'max_features': ('sqrt','log2',None),\n",
    "#}]\n",
    "\n",
    "# Extra Trees\n",
    "#hyper_params = [{'n_estimators': (10, 100, 1000,),\n",
    "#                 'min_weight_fraction_leaf': (0.0, 0.25, 0.5,),\n",
    "#                 'max_features': ('sqrt','log2','auto', None,),\n",
    "#                 'max_samples': (1,10,100,1000,),\n",
    "#                 'bootstrap': (True, False,),\n",
    "#                 'oob_score': (True, False,),\n",
    "#                 'warm_start': (True, False,),\n",
    "#                 'criterion': ('mse', 'mae',),\n",
    "#                 'max_depth': (1,10,100,None,),\n",
    "#                 'max_leaf_nodes': (1,10,100,),\n",
    "#                 'min_samples_split': (0.1,0.25,0.5,0.75,1.0,),\n",
    "#                 'min_samples_leaf': (1,10,100,),\n",
    "#}]\n",
    "\n",
    "# Support Vector Machines\n",
    "#hyper_params = [{'kernel': ('poly', 'rbf',), 'gamma': ('scale', 'auto',),\n",
    "#                 'C': (1e-2, 1e-1, 1e0, 1e1, 1e2,), 'epsilon': (1e-2, 1e-1, 1e0, 1e1, 1e2,), }]\n",
    "\n",
    "# GradientBoosting\n",
    "hyper_params = [{'n_estimators': (10, 100, 1000,),\n",
    "                 'min_weight_fraction_leaf': (0.0, 0.25, 0.5,),\n",
    "#                 'max_features': ('sqrt','log2','auto', None,),\n",
    "#                 'warm_start': (True, False,),\n",
    "#                 'criterion': ('friedman_mse', 'mse', 'mae',),\n",
    "#                 'max_depth': (1,10,100,None,),\n",
    "                 'min_samples_split': (2,5,10,100,), #0.1,0.25,0.5,0.75,1.0,),\n",
    "                 'min_samples_leaf': (2,5,10,100,),\n",
    "                 'loss': ('ls', 'lad', 'huber', 'quantile',),\n",
    "                 # 'subsample':\n",
    "                 # 'learning_rate':\n",
    "}]\n",
    "\n",
    "#est=ensemble.RandomForestRegressor()\n",
    "#est=kernel_ridge.KernelRidge()\n",
    "#est=neighbors.NearestNeighbors()\n",
    "#est=neighbors.KNeighborsRegressor()\n",
    "#est=ensemble.ExtraTreesRegressor()\n",
    "#est=svm.SVR()\n",
    "est=ensemble.GradientBoostingRegressor()\n",
    "\n",
    "gs = GridSearchCV(est, cv=5, param_grid=hyper_params, verbose=2, n_jobs=n_jobs, scoring='r2')\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(x_train, y_train.ravel())\n",
    "runtime = time.time() - t0\n",
    "print(\"Complexity and bandwidth selected and model fitted in %.6f s\" % runtime)\n",
    "\n",
    "train_score_mse = mean_squared_error(      sc_y.inverse_transform(y_train), sc_y.inverse_transform(gs.predict(x_train)))\n",
    "train_score_mae = mean_absolute_error(     sc_y.inverse_transform(y_train), sc_y.inverse_transform(gs.predict(x_train)))\n",
    "train_score_evs = explained_variance_score(sc_y.inverse_transform(y_train), sc_y.inverse_transform(gs.predict(x_train)))\n",
    "train_score_me  = max_error(               sc_y.inverse_transform(y_train), sc_y.inverse_transform(gs.predict(x_train)))\n",
    "\n",
    "test_score_mse  = mean_squared_error(      sc_y.inverse_transform(y_test),  sc_y.inverse_transform(gs.predict(x_test)))\n",
    "test_score_mae  = mean_absolute_error(     sc_y.inverse_transform(y_test),  sc_y.inverse_transform(gs.predict(x_test)))\n",
    "test_score_evs  = explained_variance_score(sc_y.inverse_transform(y_test),  sc_y.inverse_transform(gs.predict(x_test)))\n",
    "test_score_me   = max_error(               sc_y.inverse_transform(y_test),  sc_y.inverse_transform(gs.predict(x_test)))\n",
    "\n",
    "sorted_grid_params = sorted(gs.best_params_.items(), key=operator.itemgetter(0))\n",
    "\n",
    "out_text = '\\t'.join(['regression',\n",
    "                      str(trial),\n",
    "                      str(sorted_grid_params).replace('\\n',','),\n",
    "                      str(train_score_mse),\n",
    "                      str(train_score_mae),\n",
    "                      str(train_score_evs),\n",
    "                      str(train_score_me),\n",
    "                      str(test_score_mse),\n",
    "                      str(test_score_mae),\n",
    "                      str(test_score_evs),\n",
    "                      str(test_score_me),\n",
    "                      str(runtime)])\n",
    "print(out_text)\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KernelRidge\n",
    "#best_algorithm   = gs.best_params_['algorithm']\n",
    "#best_n_neighbors = gs.best_params_['n_neighbors']\n",
    "#best_leaf_size   = gs.best_params_['leaf_size']\n",
    "#best_weights     = gs.best_params_['weights']\n",
    "#best_p           = gs.best_params_['p']\n",
    "\n",
    "# kNearestNeighbour\n",
    "#best_kernel       = gs.best_params_['kernel']\n",
    "#best_alpha        = gs.best_params_['alpha']\n",
    "#best_gamma        = gs.best_params_['gamma']\n",
    "\n",
    "# RandomForest\n",
    "#best_n_estimators = gs.best_params_['n_estimators']\n",
    "#best_min_weight_fraction_leaf = gs.best_params_['min_weight_fraction_leaf']\n",
    "#best_max_features = gs.best_params_['max_features']\n",
    "\n",
    "# ExtraTrees\n",
    "#best_n_estimators = gs.best_params_['n_estimators']\n",
    "#best_min_weight_fraction_leaf = gs.best_params_['min_weight_fraction_leaf']\n",
    "#best_max_features = gs.best_params_['max_features']\n",
    "#best_max_samples = gs.best_params_['max_samples']\n",
    "#best_bootstrap = gs.best_params_['bootstrap']\n",
    "#best_oob_score = gs.best_params_['oob_score']\n",
    "#best_warm_start = gs.best_params_['warm_start']\n",
    "#best_criterion = gs.best_params_['criterion']\n",
    "#best_max_depth = gs.best_params_['max_depth']\n",
    "#best_min_samples_split = gs.best_params_['min_samples_split']\n",
    "#best_min_samples_leaf = gs.best_params_['min_samples_leaf']\n",
    "#best_max_leaf_nodes = gs.best_params_['max_leaf_nodes']\n",
    "\n",
    "# SVR\n",
    "#best_kernel = gs.best_params_['kernel']\n",
    "#best_gamma = gs.best_params_['gamma']\n",
    "#best_C = gs.best_params_['C']\n",
    "#best_epsilon = gs.best_params_['epsilon']\n",
    "\n",
    "# GB\n",
    "best_n_estimators = gs.best_params_['n_estimators']\n",
    "best_min_weight_fraction_leaf = gs.best_params_['min_weight_fraction_leaf']\n",
    "#best_max_features = gs.best_params_['max_features']\n",
    "#best_warm_start = gs.best_params_['warm_start']\n",
    "#best_criterion = gs.best_params_['criterion']\n",
    "#best_max_depth = gs.best_params_['max_depth']\n",
    "best_min_samples_split = gs.best_params_['min_samples_split']\n",
    "best_loss = gs.best_params_['loss']\n",
    "best_min_samples_leaf = gs.best_params_['min_samples_leaf']\n",
    "\n",
    "outF = open(\"output.txt\", \"w\")\n",
    "#print('best_algorithm = ', best_algorithm, file=outF)\n",
    "#print('best_n_neighbors = ', best_n_neighbors, file=outF)\n",
    "#print('best_leaf_size = ', best_leaf_size, file=outF)\n",
    "#print('best_weights = ', best_weights, file=outF)\n",
    "#print('best_p = ', best_p, file=outF)\n",
    "#\n",
    "#print('best_kernel = ', best_kernel, file=outF)\n",
    "#print('best_alpha = ', best_alpha, file=outF)\n",
    "#print('best_gamma = ', best_gamma, file=outF)\n",
    "#\n",
    "#print('best_n_estimators = ', best_n_estimators, file=outF)\n",
    "#print('best_min_weight_fraction_leaf = ', best_min_weight_fraction_leaf, file=outF)\n",
    "#print('best_max_features = ', best_max_features, file=outF)\n",
    "#\n",
    "#print('best_n_estimators = ', best_n_estimators, file=outF)\n",
    "#print('best_min_weight_fraction_leaf = ', best_min_weight_fraction_leaf, file=outF)\n",
    "#print('best_max_features = ', best_max_features, file=outF)\n",
    "#print('best_bootstrap = ', best_bootstrap, file=outF)\n",
    "#print('best_oob_score = ', best_oob_score, file=outF)\n",
    "#print('best_warm_start = ', best_warm_start, file=outF)\n",
    "#print('best_criterion = ', best_criterion, file=outF)\n",
    "#print('best_max_depth = ', best_max_depth, file=outF)\n",
    "#print('best_min_samples_split = ', best_min_samples_split, file=outF)\n",
    "#print('best_min_samples_leaf = ', best_min_samples_leaf, file=outF)\n",
    "#print('best_min_samples_leaf = ', best_min_samples_leaf, file=outF)\n",
    "#print('best_max_leaf_nodes = ', best_max_leaf_nodes, file=outF)\n",
    "#\n",
    "#print('best_kernel = ', best_kernel, file=outF)\n",
    "#print('best_gamma = ', best_gamma, file=outF)\n",
    "#print('best_C = ', best_C, file=outF)\n",
    "#print('best_epsilon = ', best_epsilon, file=outF)\n",
    "#\n",
    "print('best_n_estimators = ', best_n_estimators, file=outF)\n",
    "print('best_min_weight_fraction_leaf = ', best_min_weight_fraction_leaf, file=outF)\n",
    "#print('best_max_features = ', best_max_features, file=outF)\n",
    "#print('best_warm_start = ', best_warm_start, file=outF)\n",
    "#print('best_criterion = ', best_criterion, file=outF)\n",
    "#print('best_max_depth = ', best_max_depth, file=outF)\n",
    "print('best_min_samples_split = ', best_min_samples_split, file=outF)\n",
    "print('best_min_samples_leaf = ', best_min_samples_leaf, file=outF)\n",
    "print('best_loss = ', best_loss, file=outF)\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regr = KNeighborsRegressor(n_neighbors=best_n_neighbors, algorithm=best_algorithm,\n",
    "#                         leaf_size=best_leaf_size, weights=best_weights, p=best_p)\n",
    "#\n",
    "#regr = KernelRidge(kernel=best_kernel, gamma=best_gamma, alpha=best_alpha)\n",
    "#\n",
    "#regr = RandomForestRegressor(n_estimators=best_n_estimators,\n",
    "#                             min_weight_fraction_leaf=best_min_weight_fraction_leaf,\n",
    "#                             max_features=best_max_features)\n",
    "#\n",
    "#regr = ExtraTreesRegressor(n_estimators=best_n_estimators,\n",
    "#                           min_weight_fraction_leaf=best_min_weight_fraction_leaf,\n",
    "#                           max_features=best_max_features,\n",
    "#                           bootstrap=best_bootstrap,\n",
    "#                           oob_score=best_oob_score,\n",
    "#                           warm_start=best_warm_start,\n",
    "#                           criterion=best_criterion,\n",
    "#                           max_depth=best_max_depth,\n",
    "#                           max_leaf_nodes=best_max_leaf_nodes,\n",
    "#                           min_samples_split=best_min_samples_split,\n",
    "#                           min_samples_leaf=best_min_samples_leaf)\n",
    "#\n",
    "#regr = SVR(kernel=best_kernel, epsilon=best_epsilon, C=best_C, gamma=best_gamma)\n",
    "#\n",
    "regr = GradientBoostingRegressor(n_estimators=best_n_estimators,\n",
    "                                 min_weight_fraction_leaf=best_min_weight_fraction_leaf,\n",
    "#                                 max_features=best_max_features,\n",
    "#                                 warm_start=best_warm_start,\n",
    "#                                 criterion=best_criterion,\n",
    "#                                 max_depth=best_max_depth,\n",
    "                                 loss=best_loss,\n",
    "                                 min_samples_split=best_min_samples_split,\n",
    "                                 min_samples_leaf=best_min_samples_leaf\n",
    "                                 )\n",
    "\n",
    "t0 = time.time()\n",
    "regr.fit(x_train, y_train.ravel())\n",
    "regr_fit = time.time() - t0\n",
    "print(\"Complexity and bandwidth selected and model fitted in %.6f s\" % regr_fit)\n",
    "\n",
    "t0 = time.time()\n",
    "y_regr = regr.predict(x_test)\n",
    "regr_predict = time.time() - t0\n",
    "print(\"Prediction for %d inputs in %.6f s\" % (x_test.shape[0], regr_predict))\n",
    "\n",
    "# open a file to append\n",
    "outF = open(\"output.txt\", \"a\")\n",
    "print(\"Complexity and bandwidth selected and model fitted in %.6f s\" % regr_fit, file=outF)\n",
    "print(\"Prediction for %d inputs in %.6f s\" % (x_test.shape[0], regr_predict),file=outF)\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, y_regr), file=outF)\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, y_regr), file=outF)\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_regr)), file=outF)\n",
    "outF.close()\n",
    "\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, y_regr))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, y_regr))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_regr)))\n",
    "\n",
    "x_test_dim = sc_x.inverse_transform(x_test)\n",
    "y_test_dim = sc_y.inverse_transform(y_test)\n",
    "y_regr_dim = sc_y.inverse_transform(y_regr)\n",
    "\n",
    "#plt.scatter(x_test_dim[:,1], y_test_dim[:], s=5, c='red',     marker='o', label='KAPPA')\n",
    "#plt.scatter(x_test_dim[:,1], y_kn_dim[:],   s=2, c='magenta', marker='d', label='k-Nearest Neighbour')\n",
    "plt.scatter(x_test_dim, y_test_dim, s=5, c='r', marker='o', label='KAPPA')\n",
    "plt.scatter(x_test_dim, y_regr_dim, s=2, c='k', marker='d', label='GB')\n",
    "plt.title('Relaxation term $R_{ci}$ regression with GB')\n",
    "plt.ylabel('$R_{ci}$')\n",
    "plt.xlabel('T [K] ')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"regression_GB.pdf\", dpi=150, crop='false')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Xnew = np.array([[6750], [6800], [6850],[6900], [6950],\n",
    "#                 [7000], [7050],[7100], [7150], [7200], [7300], [7400], [7500], [7600], [7700], [7800], [7900],\n",
    "#                 [8000], [8100], [8200], [8300], [8400], [8500], [8600], [8700], [8800], [8900],\n",
    "#                 [9000], [9100], [9200], [9300], [9400], [9500], [9600], [9700], [9800], [9900],\n",
    "#                 [10000], [10100], [10200], [10300], [10400], [10500], [10757]])\n",
    "#\n",
    "#Xnew = scaler_x.transform(Xnew)\n",
    "#ynew = model.predict(Xnew)\n",
    "#\n",
    "## Invert normalize\n",
    "#ynew = scaler_y.inverse_transform(ynew)\n",
    "#Xnew = scaler_x.inverse_transform(Xnew)\n",
    "## show the inputs and predicted outputs\n",
    "#for i in range(len(Xnew)):\n",
    "#    print(\"X=%s, Predicted=%s\" % (Xnew[i], ynew[i]))\n",
    "#\n",
    "#print(x.min(), x.max())\n",
    "#\n",
    "#plt.scatter(x[:], y[:], s=15, facecolor='red', label='MATLAB')\n",
    "#plt.plot(Xnew[:], ynew[:], 'o', color='black', label='predicted', linewidth=2, markersize=5, fillstyle='none')\n",
    "#plt.title('$R_{ci}$ for $N_2/N$ and i = 10')\n",
    "#plt.ylabel('$R_{ci}$ $[J/m^3/s]$')\n",
    "#plt.xlabel('T [K] ')\n",
    "#plt.legend()\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"dim_regression.pdf\", dpi=150, crop='false')\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
