{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physics Informed Neural Network (PINN) in Tensorflow\n",
    "#\n",
    "# Task: here the NN is trained on the Euler equations for the 1d shock flow with STS approach.\n",
    "#       The vibrational populations are inferred as well as the r.h.s. source term which, in this\n",
    "#       cases includes only dissociation/recombination processes. A binary mixture N2/N is considered.\n",
    "#\n",
    "# Extensions: considering other or more complex mixture with full r.h.s. is almost immediate, \n",
    "#             just import the appropriate file and change the upper and lower boundary of data vectors.\n",
    "#\n",
    "# Known issues: with a weighting factor, w = 0 (no PINN) an excellent agreement with test dataset is obtained,\n",
    "#               but as soon as w /= 0 (PINN) the loss function degrades and the agreement gets worse ...\n",
    "#               ... infact the error associated to the Euler system increases!\n",
    "#\n",
    "#               For w = 0, the errors stay the same for different network architectures.\n",
    "#\n",
    "#               For w = 0, excellent agreement is achieved even for very shallow networks \n",
    "#               (i.e. 1 layer with 10 neurons).\n",
    "#\n",
    "# Check: dimensions of nci, Rci vectors, entering in the error and loss functions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries Note that there are 2 important data handling and\n",
    "# numerical calculation libraries: **numpy** and **scipy** alongside tensorflow.\n",
    "# *Matplotlib* is necessary to plot and visualize data\n",
    "import sys\n",
    "sys.path.insert(0, './utilities')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "#from plotting import newfig, savefig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from itertools import product, combinations\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow random seed for initialization\n",
    "np.random.seed(1234)\n",
    "tf.compat.v1.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Class PINN which we are going to use\n",
    "class PINN:\n",
    "\n",
    "    # Initialize the class\n",
    "    def __init__(self, x, nci, nat, rho, u, p, E, Rci, Rat, layers):\n",
    "\n",
    "        # Create Input Matrix for the given training data point\n",
    "        X = np.concatenate([x], 1)\n",
    "\n",
    "        # min & max for normalization\n",
    "        self.lb  = X.min(0)\n",
    "        self.ub  = X.max(0)\n",
    "        \n",
    "        self.X   = X\n",
    "\n",
    "        # class attribute definitions\n",
    "        self.x   = x\n",
    "        self.nci = nci\n",
    "        self.nat = nat\n",
    "        self.rho = rho\n",
    "        self.u   = u\n",
    "        self.p   = p\n",
    "        self.E   = E\n",
    "        self.Rci = Rci\n",
    "        self.Rat = Rat\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        # Initialize NN\n",
    "        # initialize_NN is another class method which is used to assign random\n",
    "        # weights and bias terms to the network. This not only initializes the\n",
    "        # network but also structures the sizes and values of all the weights and\n",
    "        # biases that would be so required for the network defined by layers.\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # Define a session to run\n",
    "        # tf placeholders and graph\n",
    "        self.sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n",
    "                                                                         log_device_placement=True))\n",
    "\n",
    "        # Define tensors for each variable using tf.placeholder, with shape\n",
    "        # similar to their numpy counterparts variable_Name\n",
    "        self.x_tf   = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
    "        self.rho_tf = tf.placeholder(tf.float32, shape=[None, self.rho.shape[1]])\n",
    "        self.u_tf   = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
    "        self.p_tf   = tf.placeholder(tf.float32, shape=[None, self.p.shape[1]])\n",
    "        self.E_tf   = tf.placeholder(tf.float32, shape=[None, self.E.shape[1]])\n",
    "        self.nci_tf = tf.placeholder(tf.float32, shape=[self.nci.shape[0], self.nci.shape[1]])\n",
    "        self.nat_tf = tf.placeholder(tf.float32, shape=[None, self.nat.shape[1]])\n",
    "        self.Rci_tf = tf.placeholder(tf.float32, shape=[self.Rci.shape[0], self.Rci.shape[1]])\n",
    "        self.Rat_tf = tf.placeholder(tf.float32, shape=[None, self.Rat.shape[1]])\n",
    "\n",
    "        # Predict the values of output by a single forward propagation.\n",
    "        # Also get AutoDiff coefficients from the same class method: net_Euler_STS\n",
    "        [self.nci_pred, self.nat_pred, \n",
    "         self.rho_pred, self.u_pred, self.p_pred, self.E_pred,\n",
    "         self.Rci_pred, self.Rat_pred, \n",
    "         self.e1, self.e2, self.e3, self.e4, \n",
    "         self.enci, self.enat] = self.net_Euler_STS(self.x_tf)\n",
    "\n",
    "        # MSE Normalization\n",
    "        # The initial normalization terms are necessary to ensure that the\n",
    "        # gradients don't get driven towards either the residual aquared errors\n",
    "        # or the MSE of the outputs. Basically, to ensure equal weightage to it\n",
    "        # being 'trained to training data' as well as being 'Physics informed\n",
    "        rho_norm  = np.amax(rho)\n",
    "        u_norm    = np.amax(u)\n",
    "        p_norm    = np.amax(p)\n",
    "        E_norm    = np.amax(E)\n",
    "\n",
    "        e1_norm   = rho_norm*u_norm        # e1 is continuity residual\n",
    "        e2_norm   = p_norm                 # e2 is momentum   residual\n",
    "        e3_norm   = E_norm*rho_norm*u_norm # e3 is energy     residual\n",
    "\n",
    "        nci_norm  = np.amax(nci,axis=0)\n",
    "        nat_norm  = np.amax(nat)\n",
    "        \n",
    "        Rci_norm  = np.amax(Rci,axis=0)\n",
    "        Rat_norm  = np.amax(Rat)\n",
    "\n",
    "        enci_norm = nci_norm*u_norm\n",
    "        enat_norm = nat_norm*u_norm\n",
    "        \n",
    "        # Weight factor... let's see its impact by varying it w = [0:100].\n",
    "        # If is it 0, then PINN -> NN and we do not physically inform the NN.\n",
    "        w = 0.\n",
    "\n",
    "        # Define Cost function or the Loss\n",
    "        # In this case I have set the mean squared error of the ouputs to be\n",
    "        # the loss and commented the PINN residual arguements. Uncommenting the\n",
    "        # 4 residual expressions will result in a true Phyics Informed Neural\n",
    "        # Network, otherwise, it is just a data trained Neural network\n",
    "        self.loss = tf.reduce_sum(tf.square(self.u_tf   - self.u_pred))  /(u_norm**2)   + \\\n",
    "                    tf.reduce_sum(tf.square(self.rho_tf - self.rho_pred))/(rho_norm**2) + \\\n",
    "                    tf.reduce_sum(tf.square(self.p_tf   - self.p_pred))  /(p_norm**2)   + \\\n",
    "                    tf.reduce_sum(tf.square(self.E_tf   - self.E_pred))  /(E_norm**2)   + \\\n",
    "                    tf.reduce_sum(tf.square(self.nci_tf - self.nci_pred))/(nci_norm**2) + \\\n",
    "                    tf.reduce_sum(tf.square(self.nat_tf - self.nat_pred))/(nat_norm**2) + \\\n",
    "                    tf.reduce_sum(tf.square(self.Rci_tf - self.Rci_pred))/(Rci_norm**2) + \\\n",
    "                    tf.reduce_sum(tf.square(self.Rat_tf - self.Rat_pred))/(Rat_norm**2) #+ \\\n",
    "                    #w*tf.reduce_sum(tf.square(self.e1))  /(e1_norm**2)   + \\\n",
    "                    #w*tf.reduce_sum(tf.square(self.e2))  /(e2_norm**2)   + \\\n",
    "                    #w*tf.reduce_sum(tf.square(self.e3))  /(e3_norm**2)   + \\\n",
    "                    #w*tf.reduce_sum(tf.square(self.e4))  /(p_norm**2)    + \\\n",
    "                    #w*tf.reduce_sum(tf.square(self.enci))/(enci_norm**2) + \\\n",
    "                    #w*tf.reduce_sum(tf.square(self.enat))/(enat_norm**2)\n",
    "\n",
    "        # Define optimizers\n",
    "        # There are 2 optimizers used: external by Scipy (L-BFGS-B) and internal\n",
    "        # by Tensorflow (which is Adam). The external optimizer gives an extra\n",
    "        # push after the internal has done its job. No need to change the default\n",
    "        # options of the optimizers. We have used Adam optimizer in this case,\n",
    "        # since, it is the most common and generally the fastest known converger\n",
    "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss,\n",
    "                                                                method = 'L-BFGS-B',\n",
    "                                                                options = {'maxiter': 50000,\n",
    "                                                                           'maxfun': 50000,\n",
    "                                                                           'maxcor': 50,\n",
    "                                                                           'maxls': 50,\n",
    "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
    "        # Adam\n",
    "        self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer()\n",
    "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
    "\n",
    "        # Run the session after variable initialization\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    # Class methods\n",
    "\n",
    "    # These are basic initialization functions to create the weigths and biases\n",
    "    # tensor variables and assign random values to start with code snippet\n",
    "    # iterates over the layers vector to generate the tensors as stated\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "\n",
    "    # Code for a single forward propagation pass taking in weights, biases and\n",
    "    # input matrix X. Note the normalization step on X as H before passing on to\n",
    "    # the network\n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "\n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "    # This is the differentiating code snippet which does the Automatic\n",
    "    # Differential method to find the coefficients of the necessary gradients (or\n",
    "    # equivalently, residuals) to be used in the MSE (mean squared error) in the\n",
    "    # cost function. The step is to reference the earlier function neural_net and\n",
    "    # gain the outputs as a matrix. The matrix is then sliced into individual\n",
    "    # components to get pressure, density, speed and specific energy. We define\n",
    "    # the cross section area contour S to be used in the following Physical\n",
    "    # expressions for 1D Nozzle flow. Next we find the residuals by AutoDiff.\n",
    "    # The autodiff function provided by Tensorflow is tf.gradients as above. The\n",
    "    # mass_flow_grad, momentum_grad and energy_grad are actually the residuals of\n",
    "    # the three Compressible Physical expressions. Return all the variables back\n",
    "    # to the class attributes.\n",
    "\n",
    "    def net_Euler_STS(self, x):\n",
    "\n",
    "        nci_rho_u_p_E = self.neural_net(tf.concat([x], 1), self.weights, self.biases)\n",
    "\n",
    "        nci = nci_rho_u_p_E[:,0:47]\n",
    "        nat = nci_rho_u_p_E[:,47:48]\n",
    "        rho = nci_rho_u_p_E[:,48:49]\n",
    "        u   = nci_rho_u_p_E[:,49:50]\n",
    "        p   = nci_rho_u_p_E[:,50:51]\n",
    "        E   = nci_rho_u_p_E[:,51:52]\n",
    "        Rci = nci_rho_u_p_E[:,52:99]\n",
    "        Rat = nci_rho_u_p_E[:,99:100]\n",
    "\n",
    "        # temporal derivatives\n",
    "        #nci_t   = tf.gradients(nci, t)[0]\n",
    "        #nat_t   = tf.gradients(nat, t)[0]\n",
    "\n",
    "        #rho_t   = tf.gradients(rho,   t)[0]\n",
    "        #rho_u_t = tf.gradients(rho*u, t)[0]\n",
    "        #rho_E_t = tf.gradients(rho*E, t)[0]\n",
    "\n",
    "        nci_u_x = tf.gradients(nci*u, x)[0]\n",
    "        nat_u_x = tf.gradients(nat*u, x)[0]\n",
    "\n",
    "        # autodiff gradient #1\n",
    "        mass_flow_grad = tf.gradients(rho*u, x)[0]\n",
    "\n",
    "        # autodiff gradient #2\n",
    "        momentum_grad = tf.gradients((rho*u*u + p), x)[0]\n",
    "        \n",
    "        # autodiff gradient #3\n",
    "        energy_grad = tf.gradients((rho*E + p)*u, x)[0]\n",
    "\n",
    "        # state residual\n",
    "        gamma = 1.4\n",
    "        state_res = p - rho*(gamma-1.0)*(E-0.5*gamma*u*u)\n",
    "\n",
    "        eqnci = nci_u_x - Rci\n",
    "        eqnat = nat_u_x - Rat\n",
    "\n",
    "        eq1 = mass_flow_grad\n",
    "        eq2 = momentum_grad\n",
    "        eq3 = energy_grad\n",
    "        eq4 = state_res\n",
    "\n",
    "        return nci, nat, rho, u, p, E, Rci, Rat, eq1, eq2, eq3, eq4, eqnci, eqnat\n",
    "        #return nci, nat, rho, u, p, E, eq1, eq2, eq3, eq4, eqnci, eqnat\n",
    "\n",
    "    # callback method just prints the current loss (cost) value of the network.\n",
    "    def callback(self, loss):\n",
    "        print('Loss: %.3e' % (loss))\n",
    "\n",
    "    # Train method actually trains the network weights based on the target\n",
    "    # of minimizing the loss. tf_dict is defined as the set of input and\n",
    "    # ideal output parameters for the given data in loop. For the given\n",
    "    # iterations 'nIter' (variable), the train_op_Adam session is run.\n",
    "    def train(self, nIter):\n",
    "\n",
    "        tf_dict = {self.x_tf:   self.x,\n",
    "                   self.nci_tf: self.nci, self.nat_tf: self.nat,\n",
    "                   self.rho_tf: self.rho, self.u_tf: self.u, self.p_tf: self.p, self.E_tf: self.E,\n",
    "                   self.Rci_tf: self.Rci, self.Rat_tf: self.Rat}\n",
    "\n",
    "        global loss_vector\n",
    "        loss_vector = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        for it in range(nIter):\n",
    "            self.sess.run(self.train_op_Adam, tf_dict)\n",
    "\n",
    "            #loss_value = self.sess.run(self.loss, tf_dict)\n",
    "            #loss_vector.append(loss_value)\n",
    "\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_vector.append(loss_value)\n",
    "                res1 = self.sess.run(self.e1, tf_dict)\n",
    "                res2 = self.sess.run(self.e2, tf_dict)\n",
    "                res3 = self.sess.run(self.e3, tf_dict)\n",
    "\n",
    "                #print('Iter: %d, Loss: %.3e, Time: %.2f' %\n",
    "                #      (it, loss_value, elapsed))\n",
    "                #print('Mass Residual: %f\\t\\tMomentum Residual: %f\\tEnergy Residual: %f'\n",
    "                #    %(sum(map(lambda a:a*a,res1))/len(res1), sum(map(lambda a:a*a,res2))/len(res2), sum(map(lambda a:a*a,res3))/len(res3)))\n",
    "                start_time = time.time()\n",
    "\n",
    "        # The following is external optimizer.\n",
    "        # Uncomment it to see in action. It runs indefinitely till\n",
    "        # convergence. Even after the iterations are finished, the optimizer\n",
    "        # continues to minimize the loss as compelled to by the statement\n",
    "        # self.optimizer.minimize, passing tf_dict to the loss expression defined\n",
    "        # as an attribute earlier in the class.\n",
    "        self.optimizer.minimize(self.sess,\n",
    "                                feed_dict = tf_dict,\n",
    "                                fetches = [self.loss],\n",
    "                                loss_callback = self.callback)\n",
    "\n",
    "        return loss_vector\n",
    "\n",
    "    # Predict method is used to predict the output values when test data is\n",
    "    # passed into the netowrk after the training is completed. All the values are\n",
    "    # returned to the call\n",
    "    def predict(self, x_test):\n",
    "\n",
    "        tf_dict  = {self.x_tf: x_test}\n",
    "\n",
    "        nci_test = self.sess.run(self.nci_pred, tf_dict)\n",
    "        nat_test = self.sess.run(self.nat_pred, tf_dict)\n",
    "        rho_test = self.sess.run(self.rho_pred, tf_dict)\n",
    "        u_test   = self.sess.run(self.u_pred,   tf_dict)\n",
    "        p_test   = self.sess.run(self.p_pred,   tf_dict)\n",
    "        E_test   = self.sess.run(self.E_pred,   tf_dict)\n",
    "        Rci_test = self.sess.run(self.Rci_pred, tf_dict)\n",
    "        Rat_test = self.sess.run(self.Rat_pred, tf_dict)\n",
    "\n",
    "        return nci_test, nat_test, rho_test, u_test, p_test, E_test, Rci_test, Rat_test\n",
    "        #return nci_test, nat_test, rho_test, u_test, p_test, E_test\n",
    "\n",
    "def plot_solution(X_star, u_star, index):\n",
    "\n",
    "    lb = X_star.min(0)\n",
    "    ub = X_star.max(0)\n",
    "\n",
    "    nn = 500\n",
    "\n",
    "    x = np.linspace(lb[0], ub[0], nn)\n",
    "    t = np.linspace(lb[1], ub[1], nn)\n",
    "    X, T = np.meshgrid(x,t)\n",
    "\n",
    "    U_star = griddata(X_star, u_star.flatten(), (X, T), method='cubic')\n",
    "\n",
    "    plt.figure(index)\n",
    "    plt.pcolor(X,T,U_star, cmap = 'jet')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-3b4b0761d9fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnci_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRci_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;31m# Plotting Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-647a66b76c4f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, nIter)\u001b[0m\n\u001b[1;32m    270\u001b[0m                                 \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                                 \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                                 loss_callback = self.callback)\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lk/.local/lib/python3.7/site-packages/tensorflow/contrib/opt/python/training/external_optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, session, feed_dict, fetches, step_callback, loss_callback, **run_kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mpacked_bounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packed_bounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mstep_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         optimizer_kwargs=self.optimizer_kwargs)\n\u001b[0m\u001b[1;32m    208\u001b[0m     var_vals = [\n\u001b[1;32m    209\u001b[0m         \u001b[0mpacked_var_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpacking_slice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpacking_slice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packing_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lk/.local/lib/python3.7/site-packages/tensorflow/contrib/opt/python/training/external_optimizer.py\u001b[0m in \u001b[0;36m_minimize\u001b[0;34m(self, initial_val, loss_grad_func, equality_funcs, equality_grad_funcs, inequality_funcs, inequality_grad_funcs, packed_bounds, step_callback, optimizer_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mminimize_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mminimize_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     message_lines = [\n",
      "\u001b[0;32m/home/lk/.local/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[0;32m/home/lk/.local/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lk/.local/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lk/.local/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lk/.local/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lk/.local/lib/python3.7/site-packages/tensorflow/contrib/opt/python/training/external_optimizer.py\u001b[0m in \u001b[0;36mloss_grad_func_wrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss_grad_func_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0;31m# SciPy's L-BFGS-B Fortran implementation requires gradients as doubles.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_grad_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lk/.local/lib/python3.7/site-packages/tensorflow/contrib/opt/python/training/external_optimizer.py\u001b[0m in \u001b[0;36meval_func\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0maugmented_fetch_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0maugmented_fetch_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_tensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-647a66b76c4f>\u001b[0m in \u001b[0;36mcallback\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;31m# callback method just prints the current loss (cost) value of the network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss: %.3e'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;31m# Train method actually trains the network weights based on the target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# The class PINN is initialized by using the standard init function from object\n",
    "# oriented python. There are a total of 7 variables fed to initialize:\n",
    "#\n",
    "# Inputs:\n",
    "# x (distance from the shock or through the 1D nozzle length)\n",
    "# t (time) if unsteady\n",
    "#\n",
    "# Outputs:\n",
    "# rho (density at x,t),\n",
    "# u (speed at x,t),\n",
    "# p (pressure at x,t),\n",
    "# E (specific energy at x,t),\n",
    "# layers (layers vector).\n",
    "#\n",
    "# We are providing x and t as inputs to the NN and output is P, rho, u, E\n",
    "#\n",
    "# X is the net input matrix which is formed by concatenating x and t for the\n",
    "# training data point. Additional '1' is concatenated for incorporating bias terms.\n",
    "#\n",
    "# lb and ub are the lower and upper bound respectively of X which would be later\n",
    "# used to normalize the value of X before passing it onto the neural network.\n",
    "# This is done to avoid explosion of network output values due to large training\n",
    "# data values of X.\n",
    "#\n",
    "# References:\n",
    "# https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './utilities')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from itertools import product, combinations\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "   \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Main function, inside which there are the training and testing commands.\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Neural Network Architecture\n",
    "    # layers is a vector of all the node in each of the neural network layers\n",
    "    # First value, 1 respresents the input layer with 1 parameter (x) while\n",
    "    # last value 100 is the number of outputs desired\n",
    "    #layers = [1, 10, 100]\n",
    "    layers = [1, 10, 25, 15, 100]\n",
    "    #layers = [1, 20, 20, 20, 20, 20, 20, 20, 100]\n",
    "    #layers = [1, 40, 40, 40, 40, 100]\n",
    "\n",
    "    # Load Data\n",
    "    # The Matlab generated data was stored in file name 'dataset_STS.txt', which\n",
    "    # is read by numpy and stored as a new variable by name data.\n",
    "     # x_s, n_i[:,47], n_a, rho, v, p, E, RD_mol[:,47], RD_at\n",
    "    data = np.loadtxt('data/dataset_STS.txt')\n",
    "    \n",
    "    x   = data[:,0:1]\n",
    "    nci = data[:,1:48]\n",
    "    nat = data[:,48:49]\n",
    "    \n",
    "    rho = data[:,49:50]\n",
    "    u   = data[:,50:51]\n",
    "    p   = data[:,51:52]\n",
    "    E   = data[:,52:53]\n",
    "\n",
    "    Rci = data[:,53:100]\n",
    "    Rat = data[:,100:101]\n",
    "\n",
    "    # The training set length N_train is taken to be 80% of the entire dataset.\n",
    "    # The rest 20% will be used as test set to validate the result of the training.\n",
    "    #N_train = int(0.80*data.shape[0])\n",
    "    \n",
    "    [x_train,   x_test, \n",
    "     nci_train, nci_test,\n",
    "     nat_train, nat_test,\n",
    "     rho_train, rho_test,\n",
    "     u_train,   u_test,\n",
    "     p_train,   p_test,\n",
    "     E_train,   E_test,\n",
    "     Rci_train, Rci_test,\n",
    "     Rat_train, Rat_test] = train_test_split(x, nci, nat, rho, u, p, E, Rci, Rat, test_size=0.20, random_state=0)\n",
    "        \n",
    "    # idx is a random numbers vector, which will be used to randomly pick 80% of the data from the dataset.\n",
    "    #idx = np.random.choice(range(data.shape[0]), size=(N_train,), replace=False)\n",
    "    \n",
    "    sc_x   = MinMaxScaler() #StandardScaler() RobustScaler() MaxAbsScaler()\n",
    "    sc_nci = MinMaxScaler()\n",
    "    sc_nat = MinMaxScaler()\n",
    "    sc_rho = MinMaxScaler()\n",
    "    sc_u   = MinMaxScaler()\n",
    "    sc_p   = MinMaxScaler()\n",
    "    sc_E   = MinMaxScaler()\n",
    "    sc_Rci = MinMaxScaler()\n",
    "    sc_Rat = MinMaxScaler()\n",
    "    \n",
    "    # Training set\n",
    "    x_train   = sc_x.fit_transform(x_train)\n",
    "    nci_train = sc_nci.fit_transform(nci_train)\n",
    "    nat_train = sc_nat.fit_transform(nat_train)\n",
    "    rho_train = sc_rho.fit_transform(rho_train)\n",
    "    u_train   = sc_u.fit_transform(u_train)\n",
    "    p_train   = sc_p.fit_transform(p_train)\n",
    "    E_train   = sc_E.fit_transform(E_train)\n",
    "    Rci_train = sc_Rci.fit_transform(Rci_train)\n",
    "    Rat_train = sc_Rat.fit_transform(Rat_train)\n",
    "       \n",
    "    # Testing set    \n",
    "    x_test   = sc_x.fit_transform(x_test)\n",
    "    nci_test = sc_nci.fit_transform(nci_test)\n",
    "    nat_test = sc_nat.fit_transform(nat_test)\n",
    "    rho_test = sc_rho.fit_transform(rho_test)\n",
    "    u_test   = sc_u.fit_transform(u_test)\n",
    "    p_test   = sc_p.fit_transform(p_test)\n",
    "    E_test   = sc_E.fit_transform(E_test)\n",
    "    Rci_test = sc_Rci.fit_transform(Rci_test)\n",
    "    Rat_test = sc_Rat.fit_transform(Rat_test)\n",
    "    \n",
    "    # Training the NN based on the training set, randomly chosen above model\n",
    "    # = PINN(..) passes the necessary training data to the 'NN' class (model\n",
    "    # here being an instance of the NN class) in order to initialize all the\n",
    "    # parameters as well as the NN architecture including random initialization\n",
    "    # of weights and biases.\n",
    "    #model = PINN(x, nci, nat, rho, u, p, E, Rci, Rat, layers)\n",
    "    model = PINN(x_train, nci_train, nat_train, rho_train, u_train, p_train, E_train, Rci_train, Rat_train, layers)\n",
    "\n",
    "    model.train(50000)\n",
    "    \n",
    "    # Plotting Loss\n",
    "    plt.plot(loss_vector, label='Loss value')\n",
    "    plt.legend()\n",
    "    plt.title('Loss value over iterations')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig('./figures/Loss', crop = False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Test Data\n",
    "    # Test the neural network performance using Test dataset \"data1\" generated\n",
    "    # by eliminating the initally randomly selected rows from the data. The test\n",
    "    # dataset \"data1\" is then sliced to get test parameter values.\n",
    "    #data1 = data\n",
    "    #data1 = np.delete(data1, idx, 0)\n",
    "\n",
    "    #x_test   = data1[:,0:1].flatten()[:,None]\n",
    "    #nci_test = data1[:,1:48].flatten()[:,None]\n",
    "    #nat_test = data1[:,48:49].flatten()[:,None]\n",
    "    #rho_test = data1[:,49:50].flatten()[:,None]\n",
    "    #u_test   = data1[:,50:51].flatten()[:,None]\n",
    "    #p_test   = data1[:,51:52].flatten()[:,None]\n",
    "    #E_test   = data1[:,52:53].flatten()[:,None]\n",
    "    #Rci_test = data1[:,53:100].flatten()[:,None]\n",
    "    #Rat_test = data1[:,100:101].flatten()[:,None]\n",
    "\n",
    "    # Prediction\n",
    "    # The input parameters of the test set are used to predict the pressure, density, speed and specific energy for the\n",
    "    # given x and t by using the .predict method.\n",
    "    #[nci_pred, nat_pred, rho_pred, u_pred, p_pred, E_pred, Rci_pred, Rat_pred] = model.predict(x_test)\n",
    "    [nci_pred, nat_pred, rho_pred, u_pred, p_pred, E_pred, Rci_pred, Rat_pred] = model.predict(x_test)\n",
    "\n",
    "# Error\n",
    "# Normal relative error is printed for each variable\n",
    "error_nci = np.linalg.norm(nci_test-nci_pred,2)/np.linalg.norm(nci_test,2)\n",
    "error_nat = np.linalg.norm(nat_test-nat_pred,2)/np.linalg.norm(nat_test,2)\n",
    "\n",
    "error_rho = np.linalg.norm(rho_test-rho_pred,2)/np.linalg.norm(rho_test,2)\n",
    "error_u   = np.linalg.norm(u_test-u_pred,2)/np.linalg.norm(u_test,2)\n",
    "error_p   = np.linalg.norm(p_test-p_pred,2)/np.linalg.norm(p_test,2)\n",
    "error_E   = np.linalg.norm(E_test-E_pred,2)/np.linalg.norm(E_test,2)\n",
    "\n",
    "error_Rci = np.linalg.norm(Rci_test-Rci_pred,2)/np.linalg.norm(Rci_test,2)\n",
    "error_Rat = np.linalg.norm(Rat_test-Rat_pred,2)/np.linalg.norm(Rat_test,2)\n",
    "\n",
    "print(\"Test Error in nci: \"+str(error_nci))\n",
    "print(\"Test Error in nat: \"+str(error_nat))\n",
    "print(\"Test Error in rho: \"+str(error_rho))\n",
    "print(\"Test Error in u:   \"+str(error_u))\n",
    "print(\"Test Error in p:   \"+str(error_p))\n",
    "print(\"Test Error in E:   \"+str(error_E))\n",
    "print(\"Test Error in Rci: \"+str(error_Rci))\n",
    "print(\"Test Error in Rat: \"+str(error_Rat))\n",
    "\n",
    "#x_test_plt   = data[:,0:1].flatten()[:,None]\n",
    "#nci_test_plt = data[:,1:48].flatten()[:,None]\n",
    "#nat_test_plt = data[:,48:49].flatten()[:,None]\n",
    "#rho_test_plt = data[:,49:50].flatten()[:,None]\n",
    "#u_test_plt   = data[:,50:51].flatten()[:,None]\n",
    "#p_test_plt   = data[:,51:52].flatten()[:,None]\n",
    "#E_test_plt   = data[:,52:53].flatten()[:,None]\n",
    "#Rci_test_plt = data[:,53:100].flatten()[:,None]\n",
    "#Rat_test_plt = data[:,100:101].flatten()[:,None]\n",
    "\n",
    "# Prediction (for plotting)\n",
    "#[nci_pred_plt, nat_pred_plt,\n",
    "# rho_pred_plt, u_pred_plt, p_pred_plt, E_pred_plt,\n",
    "# Rci_pred_plt, Rat_pred_plt] = model.predict(x_test_plt)\n",
    "\n",
    "# Note that these value should be changed if using a different dataset or a different time\n",
    "#c = 1\n",
    "#b = 200*c\n",
    "#a = 201\n",
    "\n",
    "x_train_sb   = sc_x.inverse_transform(x_train)\n",
    "nci_train_sb = sc_nci.inverse_transform(nci_train)\n",
    "nat_train_sb = sc_nat.inverse_transform(nat_train)\n",
    "rho_train_sb = sc_rho.inverse_transform(rho_train)\n",
    "u_train_sb   = sc_u.inverse_transform(u_train)\n",
    "p_train_sb   = sc_p.inverse_transform(p_train)\n",
    "E_train_sb   = sc_E.inverse_transform(E_train)\n",
    "Rci_train_sb = sc_Rci.inverse_transform(Rci_train)\n",
    "Rat_train_sb = sc_Rat.inverse_transform(Rat_train)\n",
    "    \n",
    "x_test_sb   = sc_x.inverse_transform(x_test)\n",
    "nci_test_sb = sc_nci.inverse_transform(nci_test)\n",
    "nat_test_sb = sc_nat.inverse_transform(nat_test)\n",
    "rho_test_sb = sc_rho.inverse_transform(rho_test)\n",
    "u_test_sb   = sc_u.inverse_transform(u_test)\n",
    "p_test_sb   = sc_p.inverse_transform(p_test)\n",
    "E_test_sb   = sc_E.inverse_transform(E_test)\n",
    "Rci_test_sb = sc_Rci.inverse_transform(Rci_test)\n",
    "Rat_test_sb = sc_Rat.inverse_transform(Rat_test)\n",
    "\n",
    "#x_pred_sb   = sc_x.inverse_transform(x_pred)\n",
    "nci_pred_sb = sc_nci.inverse_transform(nci_pred)\n",
    "nat_pred_sb = sc_nat.inverse_transform(nat_pred)\n",
    "rho_pred_sb = sc_rho.inverse_transform(rho_pred)\n",
    "u_pred_sb   = sc_u.inverse_transform(u_pred)\n",
    "p_pred_sb   = sc_p.inverse_transform(p_pred)\n",
    "E_pred_sb   = sc_E.inverse_transform(E_pred)\n",
    "Rci_pred_sb = sc_Rci.inverse_transform(Rci_pred)\n",
    "Rat_pred_sb = sc_Rat.inverse_transform(Rat_pred)\n",
    "\n",
    "# Plot Nci\n",
    "plt.plot(x_test_sb, nci_pred_sb[:,1:46:2], 'o', color='black', label='NN', linewidth=2, markersize=5, fillstyle='none')\n",
    "plt.plot(x_test_sb, nci_test_sb[:,1:46:2], 'o', color='red',   label='Exact', markersize=4)\n",
    "plt.title('Comparison of NN and Exact solution for Molecular Number Density')\n",
    "plt.xlabel('X []')\n",
    "plt.ylabel('$n_{ci}$ $[]$')\n",
    "#plt.legend()\n",
    "plt.tight_layout()\n",
    "savefig('./figures/Nci', crop = False)\n",
    "plt.show()\n",
    "\n",
    "# Plot Nat\n",
    "plt.plot(x_test_sb, nat_pred_sb, 'o', color='black', label='NN', linewidth=2, markersize=5, fillstyle='none')\n",
    "plt.plot(x_test_sb, nat_test_sb, 'o', color='red',   label='Exact', markersize=4)\n",
    "plt.title('Comparison of NN and Exact solution for Atomic Number Density')\n",
    "plt.xlabel('X []')\n",
    "plt.ylabel('$n_{at}$ $[]$')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "savefig('./figures/Nat', crop = False)\n",
    "plt.show()\n",
    "\n",
    "# Plot RHO\n",
    "plt.plot(x_test_sb, rho_pred_sb, 'o', color='black', label='NN', linewidth=2, markersize=5, fillstyle='none')\n",
    "plt.plot(x_test_sb, rho_test_sb, 'o', color='red',   label='Exact', markersize=4)\n",
    "plt.title('Comparison of NN and Exact solution for Density')\n",
    "plt.xlabel('X []')\n",
    "plt.ylabel(r'$\\rho$ $[kg/m^3]$')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "savefig('./figures/RHO', crop = False)\n",
    "plt.show()\n",
    "\n",
    "# Plot P\n",
    "plt.plot(x_test_sb, p_pred_sb, 'o', color='black', label='NN', linewidth=2, markersize=5, fillstyle='none')\n",
    "plt.plot(x_test_sb, p_test_sb, 'o', color='red',   label='Exact', markersize=4)\n",
    "plt.title('Comparison of NN and Exact solution for Pressure')\n",
    "plt.xlabel('X []')\n",
    "plt.ylabel('P [Pa]')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "savefig('./figures/P', crop = False)\n",
    "plt.show()\n",
    "\n",
    "# Plot U\n",
    "plt.plot(x_test_sb, u_pred_sb, 'o', color='black', label='NN', linewidth=2, markersize=5, fillstyle='none')\n",
    "plt.plot(x_test_sb, u_test_sb, 'o', color='red',   label='Exact', markersize=4)\n",
    "plt.title('Comparison of NN and Exact solution for Velocity')\n",
    "plt.xlabel('X []')\n",
    "plt.ylabel('U [m/s]')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "savefig('./figures/U', crop = False)\n",
    "plt.show()\n",
    "\n",
    "# Plot E\n",
    "plt.plot(x_test_sb, E_pred_sb, 'o', color='black', label='NN', linewidth=2, markersize=5, fillstyle='none')\n",
    "plt.plot(x_test_sb, E_test_sb, 'o', color='red',   label='Exact', markersize=4)\n",
    "plt.title('Comparison of NN and Exact solution for Energy')\n",
    "plt.xlabel('X []')\n",
    "plt.ylabel('E []')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "savefig('./figures/E', crop = False)\n",
    "plt.show()\n",
    "\n",
    "# Plot Rci\n",
    "plt.plot(x_test_sb, Rci_pred_sb[:,1:46:2], 'o', color='black', label='NN', linewidth=2, markersize=5, fillstyle='none')\n",
    "plt.plot(x_test_sb, Rci_test_sb[:,1:46:2], 'o', color='red',   label='Exact', markersize=4)\n",
    "plt.title('Comparison of NN and Exact solution for $R_{ci}$')\n",
    "plt.xlabel('X []')\n",
    "plt.ylabel('$R_{ci}$ $[]$')\n",
    "#plt.legend()\n",
    "plt.tight_layout()\n",
    "savefig('./figures/Rci', crop = False)\n",
    "plt.show()\n",
    "\n",
    "# Plot Nci\n",
    "plt.plot(x_test_sb, Rat_pred_sb, 'o', color='black', label='NN', linewidth=2, markersize=5, fillstyle='none')\n",
    "plt.plot(x_test_sb, Rat_test_sb, 'o', color='red',   label='Exact', markersize=4 )\n",
    "plt.title('Comparison of NN and Exact solution for $R_{at}$')\n",
    "plt.xlabel('X []')\n",
    "plt.ylabel(r'$R_{at}$ $[]$')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "savefig('./figures/Rat', crop='false')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
