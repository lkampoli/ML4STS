{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics Informed Neural Network for unsteady 1D Shock Problem\n",
    "\n",
    "# Objective\n",
    "\n",
    "* To use a Neural Network (NN) to predict an inviscid one-dimensional unsteady compressible shock flow.\n",
    "\n",
    "* To train the NN based on the input parameters: x and t and accurately predict *density* (rho), *pressure* (P), *velocity* (u) and *specific energy* (E) at that point. This would just give an ordinary deep learning network trained entirely based on the data, acting as a black box which just takes 2 inputs and produces 4 outputs. What we want is a Physics Informed Neural Network (PINN), that is, enforce the constrains defined by the governing equations by adding another term to the loss function.\n",
    "\n",
    "## Governing conservative form of Euler equation for unsteady 1D shock flow\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial [S\\textbf{u}]}{\\partial t} + \\frac{\\partial [S\\textbf{F}]}{\\partial x} - \\textbf{B}\\frac{\\partial S}{\\partial x} = 0\n",
    "\\end{equation*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation*}\n",
    "\\textbf{u} = \n",
    "\\begin{bmatrix}\n",
    "\\rho \\\\\n",
    "\\rho u \\\\\n",
    "\\rho E\n",
    "\\end{bmatrix},\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "\\textbf{F} = \n",
    "\\begin{bmatrix}\n",
    "\\rho u \\\\\n",
    "\\rho u^2 + P \\\\\n",
    "(\\rho E + P)u\n",
    "\\end{bmatrix}\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "\\textbf{B} = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "P \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "we keep the geometrical term, S, so that it may be extended to the nozzle flow. For this testcase, S = 1.\n",
    "\n",
    "### Numerical solution and dataset generation\n",
    "- The above equations are used to generate data. In the same file, the values of density, pressurem velocity and energy (or others according to the formulation) of the fluid at each prescribed point and time are pre-calculated with the code `shock1d.f90`. This data is stored in the `dataset001.txt` readable file to be used as data set for machine learning. The data is defined according to Example 1 of https://doi.org/10.1016/j.cma.2019.112789.\n",
    "\n",
    "- We want to create a neural network trained of the above data set, and also which abides the Laws of Physics as provided by the Euler equations.\n",
    "\n",
    "- The first part of the solution is simpler and consists in creating a basic deep learning framework and test your network. Note that we, first, have to divide the dataset into \"training set\" and \"test set\" and then perform the training on the training data set. The definition of the \"test set\" is quite naive, for example it could be a randomly sampled subset of the training set but other options are also reasonable.\n",
    "\n",
    "- The second part requires the \"physics\" of the problem to be taught to the network. We want the network to train, while following the governing equations at each iteration. This can be implemented by minimizing the residuals generated when the unsteady state expressions are evaluated from the predicted output.\n",
    "\n",
    "## Unsteady equation written in terms of fundamental quantitites\n",
    "NNs and PINNs are a dense network of small functions, i.e. weighted linear combinations and non-linear operators which have the capability of predicting an output variable given the \"true\"\n",
    "output value by minimising the difference, referred as loss between them. PINNs take this a level further by predicting the value of governing equations **e** in addition to the output variables by minimising the complete loss between all the predictions and their respective true values which is zero for the governing equations.\n",
    "\n",
    "The unsteady compressible equations of this problem are:\n",
    "\n",
    "$$\\frac{\\partial {(\\rho S)}}{\\partial t} + \\frac{\\partial {(\\rho uS)}}{\\partial x} = 0 \\quad\\quad\\quad\\quad\\quad\\quad\\quad(e_1) $$\n",
    "\n",
    "$$\\frac{\\partial {(\\rho uS)}}{\\partial t} + \\frac{\\partial {[(\\rho u^2 + P)S]}}{\\partial x} - p\\frac{\\partial S} {\\partial x} =0\\quad\\quad(e_2)$$\n",
    "\n",
    "$$\\frac{\\partial {(\\rho ES)}}{\\partial t} + \\frac{\\partial {(\\rho EuS)}}{\\partial x} = 0\\quad\\quad\\quad\\quad\\quad\\quad\\quad(e_3)$$\n",
    "\n",
    "The ideal gas equation is given by: $$p- \\rho {(\\gamma-1)}{(E-\\frac{u^2}{2})} = 0\\quad\\quad(e_4)$$\n",
    "\n",
    "Here, *$\\rho$* is density, *p* is pressure, *u* is velocity and *E* is specific energy.\n",
    "\n",
    "### Loss Definition for NN and PINN\n",
    "Note that, since the training set itself is dicretized and the network has inaccuracies, there will always be a residual when we actually put the values of output in the above equations, in other words, it can never be absolute 0.\n",
    "\n",
    "In addition to reducing the Mean Squared Error (MSE) of the ground truth outputs and predicted outputs of the 4 (or much more for STS) parameter variables, we try to minimize the squared error residuals (RES) generated by the unsteady state governing equations.\n",
    "\n",
    "* Loss for NN which is the mean squared error (MSE) is given by: $$NN_{Loss} = MSE = \\frac{1}{N_s}\\sum_{k=1}^{N_s} (y_{i}^{pred} -y_{i}^{truth})^2 $$\n",
    "\n",
    "  Here, *y* is an output variable and $N_s$ is the number of datapoints in the dataset.\n",
    "\n",
    "  **Note:** In case of multi-variable prediction, the MSE is summed over all the output variables.\n",
    "  \n",
    "\n",
    "* Loss for PINN is given by: $$PINN_{Loss} = MSE + RES = \\frac{1}{N_s}\\sum_{k=1}^{N_o}\\sum_{i=1}^{N_s} (y_{ik}^{pred} -y_{ik}^{truth})^2 + \\frac{1}{N_s}\\sum_{j=1}^{N_e}\\sum_{i=1}^{N_s} (e_{ij}^2)$$\n",
    "\n",
    "  Here, $N_o$ and $N_e$ reperesent the number of output variables and governing equations respectively. \n",
    "  \n",
    "  **Note:** RES is infact the MSE corresponding to *e* as $e^{truth}$ is equal to zero.\n",
    "\n",
    "\n",
    "* Ideally if the loss goes to zero, it means that all the square terms are zero individually. This means that not only is the predicted output exactly matching the corresponding ground truth but also for each data point of the dataset, the steady state governing equations are satisfied. This is the ideal PINN but, I guess, it cannot be realized due to the aforementioned reasons and because the network loss will converge to a very small value meaning that it has been trained and 'physics informed'.\n",
    "\n",
    "\n",
    "# Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lk/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lk/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lk/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lk/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lk/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lk/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/lk/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lk/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lk/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lk/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lk/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lk/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Physics Informed Neural Network (PINN) in Tensorflow\n",
    "\n",
    "# Importing necessary libraries Note that there are 2 important data handling and\n",
    "# numerical calculation libraries: **numpy** and **scipy** alongside tensorflow.\n",
    "# *Matplotlib* is necessary to plot and visualize data\n",
    "import sys\n",
    "sys.path.insert(0, './utilities')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from itertools import product, combinations\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow random seed for initialization\n",
    "np.random.seed(1234)\n",
    "tf.compat.v1.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Class PINN which we are going to use\n",
    "class PINN:\n",
    "\n",
    "    # Initialize the class\n",
    "    def __init__(self, x, t, rho, u, p, E, layers):\n",
    "\n",
    "        # Create Input Matrix for the given training data point\n",
    "        X = np.concatenate([x, t], 1)\n",
    "\n",
    "        # Domain Boundary (min & max for normalization)\n",
    "        self.lb = X.min(0)\n",
    "        self.ub = X.max(0)\n",
    "\n",
    "        self.X   = X\n",
    "\n",
    "        # Training Data (class attribute definitions)\n",
    "        self.x   = x\n",
    "        self.t   = t\n",
    "        self.rho = rho\n",
    "        self.u   = u\n",
    "        self.p   = p\n",
    "        self.E   = E\n",
    "\n",
    "        # Layers of NN\n",
    "        self.layers = layers\n",
    "\n",
    "        # Initialize NN\n",
    "        # initialize_NN is another class method which is used to assign random\n",
    "        # weights and bias terms to the network. This not only initializes the\n",
    "        # network but also structures the sizes and values of all the weights and\n",
    "        # biases that would be so required for the network defined by layers.\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Define a session to run\n",
    "        # tf placeholders and graph\n",
    "        self.sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n",
    "                                                                         log_device_placement=True))\n",
    "\n",
    "        # Define tensors for each variable using tf.placeholder, with shape\n",
    "        # similar to their numpy counterparts variable_Name\n",
    "        self.x_tf   = tf.compat.v1.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
    "        self.t_tf   = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t.shape[1]])\n",
    "        self.rho_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.rho.shape[1]])\n",
    "        self.u_tf   = tf.compat.v1.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
    "        self.p_tf   = tf.compat.v1.placeholder(tf.float32, shape=[None, self.p.shape[1]])\n",
    "        self.E_tf   = tf.compat.v1.placeholder(tf.float32, shape=[None, self.E.shape[1]])\n",
    "\n",
    "        # Predict the values of output by a single forward propagation. Also get\n",
    "        # AutoDiff coefficients from the same class method: net_Euler\n",
    "        [self.rho_pred,\n",
    "        self.u_pred,\n",
    "        self.p_pred,\n",
    "        self.E_pred,\n",
    "        self.e1,\n",
    "        self.e2,\n",
    "        self.e3,\n",
    "        self.e4] = self.net_Euler(self.x_tf, self.t_tf)\n",
    "\n",
    "        # MSE Normalization\n",
    "        # The initial normalization terms are necessary to ensure that the\n",
    "        # gradients don't get driven towards either the residual squared errors\n",
    "        # or the MSE of the outputs. Basically, to ensure equal weightage to it\n",
    "        # being 'trained to training data' as well as being 'Physics informed\n",
    "        rho_norm = np.amax(rho)\n",
    "        u_norm   = np.amax(u)\n",
    "        p_norm   = np.amax(p)\n",
    "        E_norm   = np.amax(E)\n",
    "        #S_norm  = 1.0\n",
    "        e1_norm  = rho_norm*u_norm        #*S_norm # e1 is continuity residual\n",
    "        e2_norm  = p_norm                 #*S_norm # e2 is momentum   residual\n",
    "        e3_norm  = E_norm*rho_norm*u_norm #*S_norm # e3 is energy     residual\n",
    "\n",
    "        # Weight factor... let's see its impact by varying it w = [0:100].\n",
    "        # If is it 0, then PINN -> NN and we do not physically inform the NN.\n",
    "        w = 40\n",
    "\n",
    "        # Define Cost function or the Loss\n",
    "        # In this case I have set the mean squared error of the ouputs to be\n",
    "        # the loss and commented the PINN residual arguements. Uncommenting the\n",
    "        # 4 residual expressions will result in a true Phyics Informed Neural\n",
    "        # Network, otherwise, it is just a data trained Neural network\n",
    "        # tf.reduce_mean(tf.pow(res_true - res_mom_u, 2)) TODO: try it!\n",
    "        self.loss = tf.reduce_sum(tf.square(self.p_tf   - self.p_pred))  /(p_norm**2)   + \\\n",
    "                    tf.reduce_sum(tf.square(self.rho_tf - self.rho_pred))/(rho_norm**2) + \\\n",
    "                    tf.reduce_sum(tf.square(self.u_tf   - self.u_pred))  /(u_norm**2)   + \\\n",
    "                    tf.reduce_sum(tf.square(self.E_tf   - self.E_pred))  /(E_norm**2)   #+ \\\n",
    "                    #w*tf.reduce_sum(tf.square(self.e2))/(e2_norm**2) + \\\n",
    "                    #w*tf.reduce_sum(tf.square(self.e3))/(e3_norm**2) + \\\n",
    "                    #w*tf.reduce_sum(tf.square(self.e1))/(e1_norm**2) + \\\n",
    "                    #w*tf.reduce_sum(tf.square(self.e4))/(p_norm**2)\n",
    "\n",
    "        # Define optimizers\n",
    "        # There are 2 optimizers used: external by Scipy (L-BFGS-B) and internal\n",
    "        # by Tensorflow (which is Adam). The external optimizer gives an extra\n",
    "        # push after the internal has done its job. No need to change the default\n",
    "        # options of the optimizers. We have used Adam optimizer in this case,\n",
    "        # since, it is the most common and generally the fastest known converger\n",
    "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss,\n",
    "                                                                method = 'L-BFGS-B',\n",
    "                                                                options = {'maxiter': 50000,\n",
    "                                                                           'maxfun': 50000,\n",
    "                                                                           'maxcor': 50,\n",
    "                                                                           'maxls': 50,\n",
    "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
    "        # Adam\n",
    "        #self.optimizer_Adam = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "        self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer()\n",
    "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
    "\n",
    "        # Run the session after variable initialization\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        saver.save(self.sess, 'my_test_model')\n",
    "\n",
    "    # Class methods\n",
    "\n",
    "    # These are basic initialization functions to create the weigths and biases\n",
    "    # tensor variables and assign random values to start with code snippet\n",
    "    # iterates over the layers vector to generate the tensors as stated\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "\n",
    "    # Code for a single forward propagation pass taking in weights, biases and\n",
    "    # input matrix X. Note the normalization step on X as H before passing on to\n",
    "    # the network\n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "\n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "    # This is the differentiating code snippet which does the Automatic\n",
    "    # Differential method to find the coefficients of the necessary gradients (or\n",
    "    # equivalently, residuals) to be used in the MSE (mean squared error) in the\n",
    "    # cost function. The step is to reference the earlier function neural_net and\n",
    "    # gain the outputs as a matrix. The matrix is then sliced into individual\n",
    "    # components to get pressure, density, speed and specific energy. We define\n",
    "    # the cross section area contour S to be used in the following Physical\n",
    "    # expressions for 1D Nozzle flow. Next we find the residuals by AutoDiff.\n",
    "    # The autodiff function provided by Tensorflow is tf.gradients as above. The\n",
    "    # mass_flow_grad, momentum_grad and energy_grad are actually the residuals of\n",
    "    # the three Compressible Physical expressions. Return all the variables back\n",
    "    # to the class attributes.\n",
    "    def net_Euler(self, x, t):\n",
    "\n",
    "        rho_u_p_E = self.neural_net(tf.concat([x,t], 1), self.weights, self.biases)\n",
    "\n",
    "        rho = rho_u_p_E[:,0:1]\n",
    "        u   = rho_u_p_E[:,1:2]\n",
    "        p   = rho_u_p_E[:,2:3]\n",
    "        E   = rho_u_p_E[:,3:4]\n",
    "\n",
    "        #S = 1.0 + 2.2*((x-50)**2)*(9/10000)\n",
    "\n",
    "        # temporal derivatives\n",
    "        rho_t   = tf.gradients(rho,   t)[0]\n",
    "        rho_u_t = tf.gradients(rho*u, t)[0]\n",
    "        rho_E_t = tf.gradients(rho*E, t)[0]\n",
    "\n",
    "        # autodiff gradient #1\n",
    "        mass_flow_grad = tf.gradients(rho*u, x)[0]\n",
    "\n",
    "        # autodiff gradient #2\n",
    "        momentum_grad = tf.gradients((rho*u*u + p), x)[0]\n",
    "\n",
    "        # autodiff gradient #3\n",
    "        energy_grad = tf.gradients((rho*E + p)*u, x)[0]\n",
    "\n",
    "        # state residual\n",
    "        gamma = 1.4\n",
    "        state_res = p - rho*(gamma-1.0)*(E-0.5*gamma*u*u)\n",
    "\n",
    "        eq1 = rho_t   + mass_flow_grad\n",
    "        eq2 = rho_u_t + momentum_grad\n",
    "        eq3 = rho_E_t + energy_grad\n",
    "        eq4 =           state_res\n",
    "        \n",
    "        return rho, u, p, E, eq1, eq2, eq3, eq4\n",
    "\n",
    "    # callback method just prints the current loss (cost) value of the network.\n",
    "    def callback(self, loss):\n",
    "        print('Loss: %.3e' % (loss))\n",
    "\n",
    "    # Train method actually trains the network weights based on the target\n",
    "    # of minimizing the loss. tf_dict is defined as the set of input and\n",
    "    # ideal output parameters for the given data in loop. For the given\n",
    "    # iterations 'nIter' (variable), the train_op_Adam session is run.\n",
    "    def train(self, nIter):\n",
    "\n",
    "        tf_dict = {self.x_tf: self.x,\n",
    "                   self.t_tf: self.t,\n",
    "                   self.rho_tf: self.rho,\n",
    "                   self.u_tf: self.u,\n",
    "                   self.p_tf: self.p,\n",
    "                   self.E_tf: self.E}\n",
    "\n",
    "        global loss_vector\n",
    "        loss_vector = []\n",
    "\n",
    "        start_time = time.time()   \n",
    "        for it in range(nIter):\n",
    "            self.sess.run(self.train_op_Adam, tf_dict)\n",
    "\n",
    "            loss_value = self.sess.run(self.loss, tf_dict)\n",
    "            loss_vector.append(loss_value)\n",
    "\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                #loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                res1 = self.sess.run(self.e1, tf_dict)\n",
    "                res2 = self.sess.run(self.e2, tf_dict)\n",
    "                res3 = self.sess.run(self.e3, tf_dict)\n",
    "  \n",
    "                print('Iter: %d, Loss: %.3e, Time: %.2f' %\n",
    "                      (it, loss_value, elapsed))\n",
    "                print('Mass Residual: %f\\t\\tMomentum Residual: %f\\tEnergy Residual: %f'\n",
    "                    %(sum(map(lambda a:a*a,res1))/len(res1), sum(map(lambda a:a*a,res2))/len(res2), sum(map(lambda a:a*a,res3))/len(res3)))\n",
    "                start_time = time.time()\n",
    "\n",
    "        # The following is external optimizer.\n",
    "        # Uncomment it to see in action. It runs indefinitely till\n",
    "        # convergence. Even after the iterations are finished, the optimizer\n",
    "        # continues to minimize the loss as compelled to by the statement\n",
    "        # self.optimizer.minimize, passing tf_dict to the loss expression defined\n",
    "        # as an attribute earlier in the class.\n",
    "        self.optimizer.minimize(self.sess,\n",
    "                feed_dict = tf_dict,\n",
    "                fetches = [self.loss],\n",
    "                loss_callback = self.callback)\n",
    "\n",
    "        return loss_vector\n",
    "    \n",
    "    \n",
    "    # A more flexible train implementation\n",
    "    def train2(self, num_epochs, batch_size, learning_rate):                                                                         \n",
    "                                                                                                                                      \n",
    "        for epoch in range(num_epochs):                                                                                             \n",
    "                                                                                                                                      \n",
    "            N = self.t.shape[0]                                                                                                     \n",
    "            perm = np.random.permutation(N)                                                                                         \n",
    "                                                                                                                                      \n",
    "            start_time = time.time()                                                                                                \n",
    "            for it in range(0, N, batch_size):                                                                                      \n",
    "                idx = perm[np.arange(it,it+batch_size)]                                                                             \n",
    "                (x_batch,                                                                                                           \n",
    "                t_batch,                                                                                                           \n",
    "                rho_batch,                                                                                                           \n",
    "                u_batch,                                                                                                           \n",
    "                p_batch,                                                                                                           \n",
    "                E_batch) = (self.x[idx,:],                                                                                       \n",
    "                            self.t[idx,:],                                                                                       \n",
    "                            self.rho[idx,:],                                                                                       \n",
    "                            self.u[idx,:],                                                                                       \n",
    "                            self.p[idx,:],                                                                                       \n",
    "                            self.E[idx,:])                                                                                     \n",
    "                                                                                                                                      \n",
    "                tf_dict = {self.x_tf: x_batch, self.t_tf: t_batch, self.rho_tf: rho_batch,                                              \n",
    "                           self.u_tf: u_batch, self.p_tf: p_batch, self.E_tf: E_batch,                                          \n",
    "                           self.dummy_tf: np.ones((batch_size, self.layers[-1])),                                               \n",
    "                           self.learning_rate: learning_rate}                                                                       \n",
    "                                                                                                                                       \n",
    "                self.sess.run(self.train_op, tf_dict)                                                                               \n",
    "                                                                                                                                       \n",
    "                # Print                                                                                                             \n",
    "                if it % (10*batch_size) == 0:                                                                                       \n",
    "                    elapsed = time.time() - start_time                                                                              \n",
    "                    loss_value, learning_rate_value = self.sess.run([self.loss,self.learning_rate], tf_dict)                        \n",
    "                    print('Epoch: %d, It: %d, Loss: %.3e, Time: %.2f, Learning Rate: %.3e'                                          \n",
    "                          %(epoch, it/batch_size, loss_value, elapsed, learning_rate_value))                                        \n",
    "                    start_time = time.time()\n",
    "        \n",
    "    # Predict method is used to predict the output values when test data is\n",
    "    # passed into the netowrk after the training is completed. All the values are\n",
    "    # returned to the call\n",
    "    def predict(self, x_test, t_test):\n",
    "\n",
    "        tf_dict  = {self.x_tf: x_test, self.t_tf: t_test}\n",
    "\n",
    "        rho_test = self.sess.run(self.rho_pred, tf_dict)\n",
    "        u_test   = self.sess.run(self.u_pred,   tf_dict)\n",
    "        p_test   = self.sess.run(self.p_pred,   tf_dict)\n",
    "        E_test   = self.sess.run(self.E_pred,   tf_dict)\n",
    "\n",
    "        return rho_test, u_test, p_test, E_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_solution(X_star, u_star, index):\n",
    "\n",
    "    lb = X_star.min(0)\n",
    "    ub = X_star.max(0)\n",
    "\n",
    "    nn = 750\n",
    "\n",
    "    x = np.linspace(lb[0], ub[0], nn)\n",
    "    t = np.linspace(lb[1], ub[1], nn)\n",
    "    X, T = np.meshgrid(x,t)\n",
    "\n",
    "    U_star = griddata(X_star, u_star.flatten(), (X, T), method='cubic')\n",
    "\n",
    "    plt.figure(index)\n",
    "    plt.pcolor(X,T,U_star, cmap = 'jet')\n",
    "    plt.colorbar()\n",
    "    #savefig('./figures/fig_pred', crop = False)\n",
    "    #plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Iter: 0, Loss: 5.671e+04, Time: 0.45\n",
      "Mass Residual: 0.001317\t\tMomentum Residual: 0.082210\tEnergy Residual: 0.000039\n",
      "Iter: 100, Loss: 3.409e+02, Time: 5.24\n",
      "Mass Residual: 0.001926\t\tMomentum Residual: 0.003449\tEnergy Residual: 0.014755\n",
      "Iter: 200, Loss: 2.495e+02, Time: 5.64\n",
      "Mass Residual: 0.001230\t\tMomentum Residual: 0.012208\tEnergy Residual: 0.012637\n",
      "Iter: 300, Loss: 2.251e+02, Time: 5.73\n",
      "Mass Residual: 0.000750\t\tMomentum Residual: 0.010844\tEnergy Residual: 0.012793\n",
      "Iter: 400, Loss: 2.055e+02, Time: 6.29\n",
      "Mass Residual: 0.000609\t\tMomentum Residual: 0.008029\tEnergy Residual: 0.011519\n",
      "Iter: 500, Loss: 1.858e+02, Time: 5.28\n",
      "Mass Residual: 0.000508\t\tMomentum Residual: 0.005805\tEnergy Residual: 0.010452\n",
      "Iter: 600, Loss: 1.624e+02, Time: 5.91\n",
      "Mass Residual: 0.000436\t\tMomentum Residual: 0.004169\tEnergy Residual: 0.009950\n",
      "Iter: 700, Loss: 1.305e+02, Time: 6.27\n",
      "Mass Residual: 0.000374\t\tMomentum Residual: 0.003297\tEnergy Residual: 0.010497\n",
      "Iter: 800, Loss: 9.374e+01, Time: 5.96\n",
      "Mass Residual: 0.000347\t\tMomentum Residual: 0.003008\tEnergy Residual: 0.010507\n",
      "Iter: 900, Loss: 8.177e+01, Time: 5.78\n",
      "Mass Residual: 0.000401\t\tMomentum Residual: 0.002451\tEnergy Residual: 0.008605\n"
     ]
    }
   ],
   "source": [
    "# The class PINN is initialized by using the standard init function from object\n",
    "# oriented python. There are a total of 7 variables fed to initialize:\n",
    "#\n",
    "# Inputs:\n",
    "# x (distance from the shock or through the 1D nozzle length)\n",
    "# t (time)\n",
    "#\n",
    "# Outputs:\n",
    "# rho (density at x,t),\n",
    "# u (speed at x,t),\n",
    "# p (pressure at x,t),\n",
    "# E (specific energy at x,t),\n",
    "# layers (layers vector).\n",
    "#\n",
    "# We are providing x and t as inputs to the NN and output is P, rho, u, E\n",
    "#\n",
    "# X is the net input matrix which is formed by concatenating x and t for the\n",
    "# training data point. Additional '1' is concatenated for incorporating bias terms.\n",
    "#\n",
    "# lb and ub are the lower and upper bound respectively of X which would be later\n",
    "# used to normalize the value of X before passing it onto the neural network.\n",
    "# This is done to avoid explosion of network output values due to large training\n",
    "# data values of X.\n",
    "\n",
    "# Main function, inside which there are the training and testing commands.\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Neural Network Architecture\n",
    "    # layers is a vector of all the node in each of the neural network layers\n",
    "    # First value, 2 respresents the input layer with 2 parameters while\n",
    "    # last value 4 is the number of outputs desired\n",
    "    #layers = [2, 10, 25, 15, 4]\n",
    "    layers = [2, 20, 20, 20, 20, 20, 20, 20, 4] # NICE!\n",
    "    #layers = [2, 40, 40, 40, 40, 4]\n",
    "\n",
    "    # Load Data\n",
    "    # The Fortran generated data was stored in file name 'datashcok1d.txt', which\n",
    "    # is read by numpy and stored as a new variable by name data.\n",
    "    #data = np.loadtxt('data/datashock1d.txt') # too big\n",
    "    data = np.loadtxt('data/datashock1d001.txt')\n",
    "    #data = np.loadtxt('data/datashock1dlite.txt') # too small\n",
    "\n",
    "    # The training set length N_train is taken to be 85% of the entire dataset.\n",
    "    # The rest 15% will be used as test set to validate the result of the training.\n",
    "    N_train = int(0.85*data.shape[0])\n",
    "\n",
    "    # idx is a random numbers vector, which will be used to randomly pick 85% of\n",
    "    # the data from the dataset.\n",
    "    idx = np.random.choice(range(data.shape[0]), size=(N_train,), replace=False)\n",
    "\n",
    "    # The rest is mere slicing of dataset to get all required parameters.\n",
    "    # x\n",
    "    x_train  = data[idx,0:1].flatten()[:,None]\n",
    "    t_train  = data[idx,1:2].flatten()[:,None]\n",
    "    XT_train = np.concatenate([x_train, t_train], 1)\n",
    "    #X, T    = np.meshgrid(x_train,t_train)\n",
    "    # y\n",
    "    rho_train = data[idx,2:3].flatten()[:,None]\n",
    "    u_train   = data[idx,3:4].flatten()[:,None]\n",
    "    p_train   = data[idx,4:5].flatten()[:,None]\n",
    "    E_train   = data[idx,5:6].flatten()[:,None]\n",
    "\n",
    "    # Training the NN based on the training set, randomly chosen above model\n",
    "    # = PINN(..) passes the necessary training data to the 'NN' class (model\n",
    "    # here being an instance of the NN class) in order to initialize all the\n",
    "    # parameters as well as the NN architecture including random initialization\n",
    "    # of weights and biases.\n",
    "    model = PINN(x_train, t_train, rho_train, u_train, p_train, E_train, layers)\n",
    "    model.train(20000) \n",
    "    \n",
    "    #model.train2(num_epochs = 200, batch_size = 10000, learning_rate=1e-3)                                                           \n",
    "    #model.train2(num_epochs = 300, batch_size = 10000, learning_rate=1e-4)                                                           \n",
    "    #model.train2(num_epochs = 300, batch_size = 10000, learning_rate=1e-5)                                                           \n",
    "    #model.train2(num_epochs = 200, batch_size = 10000, learning_rate=1e-6)\n",
    "    \n",
    "    # Plotting Loss\n",
    "    plt.plot(loss_vector, label='Loss value')\n",
    "    plt.legend()\n",
    "    plt.title('Loss value over iterations')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    savefig('./figures/Loss', crop = False)\n",
    "    plt.show()\n",
    "\n",
    "    # Test Data\n",
    "    # Test the neural network performance using Test dataset \"data1\" generated\n",
    "    # by eliminating the initally randomly selected rows from the data. The test\n",
    "    # dataset \"data1\" is then sliced to get test parameter values.\n",
    "    data1 = data\n",
    "    data1 = np.delete(data1, idx, 0)\n",
    "    # x\n",
    "    x_test  = data1[:,0:1].flatten()[:,None]\n",
    "    t_test  = data1[:,1:2].flatten()[:,None]\n",
    "    XT_test = np.concatenate([x_test, t_test], 1)\n",
    "    # y\n",
    "    rho_test = data1[:,2:3].flatten()[:,None]\n",
    "    u_test   = data1[:,3:4].flatten()[:,None]\n",
    "    p_test   = data1[:,4:5].flatten()[:,None]\n",
    "    E_test   = data1[:,5:6].flatten()[:,None]\n",
    "\n",
    "    # Prediction\n",
    "    # The input parameters of the test set are used to predict the pressure,\n",
    "    # density, speed and specific energy for the given x and t by using the\n",
    "    # .predict method.\n",
    "    rho_pred, u_pred, p_pred, E_pred = model.predict(x_test, t_test)\n",
    "\n",
    "    #RHO_pred = griddata(x_test, rho_pred.flatten(), (X, T), method='cubic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Error\n",
    "    # Normal relative error is printed for each variable\n",
    "    error_rho = np.linalg.norm(rho_test-rho_pred,2)/np.linalg.norm(rho_test,2)\n",
    "    error_u = np.linalg.norm(u_test-u_pred,2)/np.linalg.norm(u_test,2)\n",
    "    error_p = np.linalg.norm(p_test-p_pred,2)/np.linalg.norm(p_test,2)\n",
    "    error_E = np.linalg.norm(E_test-E_pred,2)/np.linalg.norm(E_test,2)\n",
    "    print(\"Test Error in rho: \"+str(error_rho))\n",
    "    print(\"Test Error in u: \"+str(error_u))\n",
    "    print(\"Test Error in p: \"+str(error_p))\n",
    "    print(\"Test Error in E: \"+str(error_E))\n",
    "    \n",
    "    #errors = np.sqrt((Y_test-Y_pred)**2/Y_test**2)                                                                                  \n",
    "    #mean_errors = np.mean(errors,0)                                                                                                 \n",
    "    #std_errors = np.std(errors,0)\n",
    "\n",
    "    #Plotting\n",
    "    #a = 201\n",
    "    #for i in range (0, 3046, 1608):\n",
    "    #plt.plot(x_test[0:3046:a], rho_pred[0:3046:a], 'm', label='NN')\n",
    "    #plt.plot(x_test[0:3046:a], rho_test[0:3046:a], 'g', label='Exact')\n",
    "    #plt.title('Comparison of NN and Exact solution for Density')\n",
    "    #plt.xlabel('x')\n",
    "    #plt.ylabel('value')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "    #savefig('./figures/Error', crop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Plotting\n",
    "    plot_solution(XT_test, rho_pred, 1)\n",
    "    savefig('./figures/rho_pred', crop = False)\n",
    "    plt.show()   \n",
    "    plot_solution(XT_test, u_pred, 1)\n",
    "    savefig('./figures/u_pred', crop = False)\n",
    "    plt.show()   \n",
    "    plot_solution(XT_test, p_pred, 1)\n",
    "    savefig('./figures/p_pred', crop = False)\n",
    "    plt.show()   \n",
    "    plot_solution(XT_test, E_pred, 1)\n",
    "    savefig('./figures/E_pred', crop = False)\n",
    "    plt.show()   \n",
    "\n",
    "    # Predict for plotting\n",
    "    #lb = XT_test.min(0)\n",
    "    #ub = XT_test.max(0)\n",
    "    #nn = 100\n",
    "    #x = np.linspace(lb[0], ub[0], nn)\n",
    "    #t = np.linspace(lb[1], ub[1], nn)\n",
    "    #X, T = np.meshgrid(x,t)\n",
    "\n",
    "    #RR_star = griddata(XT_test, rho_pred.flatten(), (X, T), method='cubic')\n",
    "    #UU_star = griddata(XT_test, u_pred.flatten(),   (X, T), method='cubic')\n",
    "    #PP_star = griddata(XT_test, p_pred.flatten(),   (X, T), method='cubic')\n",
    "    #EE_star = griddata(XT_test, E_pred.flatten(),   (X, T), method='cubic')\n",
    "\n",
    "#    ############################# Plotting ###############################\n",
    "#\n",
    "#    fig, ax = newfig(1.0, 1.4)\n",
    "#    ax.axis('off')\n",
    "#\n",
    "#    ####### Row 0: rho(t,x) ##################\n",
    "#    gs0 = gridspec.GridSpec(1, 2)\n",
    "#    gs0.update(top=1-0.06, bottom=1-1.0/3.0+0.06, left=0.15, right=0.85, wspace=0)\n",
    "#    ax = plt.subplot(gs0[:, :])\n",
    "#\n",
    "#    h = ax.imshow(RR_star.T, interpolation='nearest', cmap='rainbow',\n",
    "#                  #extent=[lb[0], ub[0], lb[1], ub[1]], #t_test.min(),t_test.max()\n",
    "#                  #extent=[lb[1], ub[1], lb[0], ub[0]], #t_test.min(),t_test.max()\n",
    "#                  extent=[t_test.min(),t_test.max(), lb[0], ub[0]],\n",
    "#                  origin='lower', aspect='auto')\n",
    "#    divider = make_axes_locatable(ax)\n",
    "#    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "#    fig.colorbar(h, cax=cax)\n",
    "#\n",
    "#    ax.plot(XT_train[:,0], XT_train[:,1], 'kx', label = 'Data (%d points)' % (rho_train.shape[0]), markersize = 2, clip_on = False)\n",
    "#\n",
    "#    line = np.linspace(x_train.min(), x_train.max(), 2)[:,None]\n",
    "#    #ax.plot(t_test[5]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "#    #ax.plot(t_test[15]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "#    #ax.plot(t_test[30]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "#\n",
    "#    ax.set_xlabel('$x$')\n",
    "#    ax.set_ylabel('$t$')\n",
    "#    ax.legend(loc='upper center', bbox_to_anchor=(1.0, -0.125), ncol=5, frameon=False)\n",
    "#    #leg = ax.legend(frameon=False, loc = 'best')\n",
    "#    #plt.setp(leg.get_texts(), color='w')\n",
    "#    ax.set_title(r'$\\rho(x,t)$', fontsize = 10)\n",
    "#\n",
    "#    ####### Row 1: rho(t,x) slices ##################\n",
    "#    gs1 = gridspec.GridSpec(1, 4)\n",
    "#    gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
    "#\n",
    "#    ax = plt.subplot(gs1[0, 0])\n",
    "#    # ax.plot(x_test,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "#    # ax.plot(x_test,rho_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "#    ax.set_xlabel('$x$')\n",
    "#    ax.set_ylabel('Density')\n",
    "#    ax.set_title('$t = 2.0$', fontsize = 10)\n",
    "#    ax.axis('square')\n",
    "#    ax.set_xlim([0.0,1.0])\n",
    "#    ax.set_ylim([0.0,2.0])\n",
    "#\n",
    "#    ax = plt.subplot(gs1[0, 1])\n",
    "#    # ax.plot(x_test,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "#    # ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "#    ax.set_xlabel('$x$')\n",
    "#    ax.set_ylabel('Velocity')\n",
    "#    ax.axis('square')\n",
    "#    ax.set_xlim([0.0,1.0])\n",
    "#    ax.set_ylim([0.0,2.0])\n",
    "#    ax.set_title('$t = 2.0$', fontsize = 10)\n",
    "#    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "#\n",
    "#    ax = plt.subplot(gs1[0, 2])\n",
    "#    #ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "#    #ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "#    ax.set_xlabel('$x$')\n",
    "#    ax.set_ylabel('Pressure')\n",
    "#    ax.axis('square')\n",
    "#    ax.set_xlim([0.0,1.0])\n",
    "#    ax.set_ylim([0.0, 2.0])\n",
    "#    ax.set_title('$t = 2.0$', fontsize = 10)\n",
    "#\n",
    "#    ax = plt.subplot(gs1[0, 3])\n",
    "#    # ax.plot(x_test,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "#    # ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "#    ax.set_xlabel('$x$')\n",
    "#    ax.set_ylabel('Energy')\n",
    "#    ax.axis('square')\n",
    "#    ax.set_xlim([0.0,1.0])\n",
    "#    ax.set_ylim([0.0,2.0])\n",
    "#    ax.set_title('$t = 2.0$', fontsize = 10)\n",
    "#    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "#\n",
    "#    savefig('./figures/Euler_identification')\n",
    "\n",
    "    # Visualize and compare the results against CFD generated outputs: plt_u and\n",
    "    # plt_l are the upper and lower bounds of a visualizing window respectively.\n",
    "    # This defines a specific set of inputs for which the results will be plotted.\n",
    "    # Again, we predict the output values for the specific input window. We now\n",
    "    # plot the predicted ouputs by the network against the exact solution values \n",
    "    # by CFD using matplotlib\n",
    "\n",
    "    # x\n",
    "    x_test_plt  = data[:, 0:1].flatten()[:,None]\n",
    "    t_test_plt  = data[:, 1:2].flatten()[:,None]\n",
    "    \n",
    "    # y\n",
    "    rho_test_plt = data[:, 2:3].flatten()[:,None]\n",
    "    u_test_plt   = data[:, 3:4].flatten()[:,None]\n",
    "    p_test_plt   = data[:, 4:5].flatten()[:,None]\n",
    "    E_test_plt   = data[:, 5:6].flatten()[:,None]\n",
    "\n",
    "    # Prediction (for plotting)\n",
    "    rho_pred_plt, u_pred_plt, p_pred_plt, E_pred_plt = model.predict(x_test_plt, t_test_plt)\n",
    "    \n",
    "    # Note that these value should be changed if using a different dataset or a different time\n",
    "    c = 1\n",
    "    b = 200*c\n",
    "    a = 201\n",
    "    \n",
    "    # Plot RHO\n",
    "    plt.plot(x_test_plt[0+b:len(x_test_plt)-1:a], rho_pred_plt[0+b:len(x_test_plt)-1:a], 'm', label='NN')\n",
    "    plt.plot(x_test_plt[0+b:len(x_test_plt)-1:a], rho_test_plt[0+b:len(x_test_plt)-1:a], 'g', label='Exact')\n",
    "    plt.title('Comparison of NN and Exact solution for Density at t = ' +str(t_test_plt[b]))\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('value')\n",
    "    plt.legend()\n",
    "    savefig('./figures/predictedRHO', crop = False)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot P\n",
    "    plt.plot(x_test_plt[0+b:len(x_test_plt)-1:a], p_pred_plt[0+b:len(x_test_plt)-1:a], 'm', label='NN')\n",
    "    plt.plot(x_test_plt[0+b:len(x_test_plt)-1:a], p_test_plt[0+b:len(x_test_plt)-1:a], 'g', label='Exact')\n",
    "    plt.title('Comparison of NN and Exact solution for Pressure at t = ' +str(t_test_plt[b]))\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('value')\n",
    "    plt.legend()\n",
    "    savefig('./figures/predictedP', crop = False)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot U\n",
    "    plt.plot(x_test_plt[0+b:len(x_test_plt)-1:a], u_pred_plt[0+b:len(x_test_plt)-1:a], 'm', label='NN')\n",
    "    plt.plot(x_test_plt[0+b:len(x_test_plt)-1:a], u_test_plt[0+b:len(x_test_plt)-1:a], 'g', label='Exact')\n",
    "    plt.title('Comparison of NN and Exact solution for Velocity at t = ' +str(t_test_plt[b]))\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('value')\n",
    "    plt.legend()\n",
    "    savefig('./figures/predictedU', crop = False)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot E\n",
    "    plt.plot(x_test_plt[0+b:len(x_test_plt)-1:a], E_pred_plt[0+b:len(x_test_plt)-1:a], 'm', label='NN')\n",
    "    plt.plot(x_test_plt[0+b:len(x_test_plt)-1:a], E_test_plt[0+b:len(x_test_plt)-1:a], 'g', label='Exact')\n",
    "    plt.title('Comparison of NN and Exact solution for Energy at t = ' +str(t_test_plt[b]))\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('value')\n",
    "    plt.legend()\n",
    "    savefig('./figures/predictedE', crop = False)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
