{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "sys.path.insert(0, './')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from itertools import product, combinations\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Tensorflow random seed for initialization\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "\n",
    "# Define the Class PINN which we are going to use\n",
    "class PINN:\n",
    "\n",
    "    # Initialize the class\n",
    "    def __init__(self, x, n, rho, u, p, E, R, layers):\n",
    "\n",
    "        # Create Input Matrix for the given training data point\n",
    "        X = np.concatenate([x], 1)\n",
    "\n",
    "        length = len(n)\n",
    "        # min & max for normalization\n",
    "        self.lb = X.min(0)\n",
    "        self.ub = X.max(0)\n",
    "\n",
    "        self.X  = X\n",
    "\n",
    "        # class attribute definitions\n",
    "        self.x = x\n",
    "\n",
    "        self.n = []\n",
    "        i=0\n",
    "        while i<length:\n",
    "          self.n.append(n[i])\n",
    "          i=i+1\n",
    "\n",
    "        self.rho = rho\n",
    "        self.u   = u\n",
    "        self.p   = p\n",
    "        self.E   = E\n",
    "\n",
    "        self.R = []\n",
    "        i = 0\n",
    "        while i<length:\n",
    "          self.R.append(R[i])\n",
    "          i=i+1\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        # Initialize_NN is another class method which is used to assign random\n",
    "        # weights and bias terms to the network. This not only initializes the\n",
    "        # network but also structures the sizes and values of all the weights and\n",
    "        # biases that would be so required for the network defined by layers.\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # Define a session to run\n",
    "        # tf placeholders and graph\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                                         log_device_placement=True))\n",
    "\n",
    "        # Define tensors for each variable using tf.placeholder, with shape\n",
    "        # similar to their numpy counterparts variable_Name\n",
    "        self.x_tf   = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
    "\n",
    "        self.rho_tf = tf.placeholder(tf.float32, shape=[None, self.rho.shape[1]])\n",
    "        self.u_tf   = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
    "        self.p_tf   = tf.placeholder(tf.float32, shape=[None, self.p.shape[1]])\n",
    "        self.E_tf   = tf.placeholder(tf.float32, shape=[None, self.E.shape[1]])\n",
    "\n",
    "        self.n_tf = [tf.placeholder(tf.float32, shape=[None, self.n[i].shape[1]]) for i in range(int(48))]\n",
    "        self.R_tf = [tf.placeholder(tf.float32, shape=[None, self.R[i].shape[1]]) for i in range(int(48))]\n",
    "\n",
    "        # Predict the values of output by a single forward propagation.\n",
    "        # Also get AutoDiff coefficients from the same class method: net_Euler_STS\n",
    "        [self.n_pred,\n",
    "         #####\n",
    "         self.rho_pred, self.u_pred, self.p_pred, self.E_pred,\n",
    "         #####\n",
    "         self.R_pred,\n",
    "         #####\n",
    "         self.e1, self.e2, self.e3, self.e4,\n",
    "         #####\n",
    "         self.en] = self.net_Euler_STS(self.x_tf)\n",
    "\n",
    "        # MSE Normalization\n",
    "        # The initial normalization terms are necessary to ensure that the\n",
    "        # gradients don't get driven towards either the residual squared errors\n",
    "        # or the MSE of the outputs. Basically, to ensure equal weightage to it\n",
    "        # being 'trained to training data' as well as being 'Physics informed'\n",
    "        rho_norm = np.amax(rho)\n",
    "        u_norm   = np.amax(u)\n",
    "        p_norm   = np.amax(p)\n",
    "        E_norm   = np.amax(E)\n",
    "\n",
    "        e1_norm  = rho_norm*u_norm        # e1 is continuity residual\n",
    "        e2_norm  = p_norm                 # e2 is momentum   residual\n",
    "        e3_norm  = E_norm*rho_norm*u_norm # e3 is energy     residual\n",
    "\n",
    "        n_norm = [np.amax(n[i]) for i in range(int(48))]\n",
    "        en_norm = [n_norm[i]*u_norm for i in range(int(48))]\n",
    "        R_norm = [np.amax(R[i]) for i in range(int(48))]\n",
    "\n",
    "\n",
    "        # Weight factor... let's see its impact by varying it w = [0:100].\n",
    "        # If is it 0, then PINN -> NN and we do not physically inform the NN.\n",
    "        w = 0.0\n",
    "\n",
    "        # Define Cost function or the Loss\n",
    "        # In this case I have set the mean squared error of the ouputs to be\n",
    "        # the loss and commented the PINN residual arguements. Uncommenting the\n",
    "        # residual expressions will result in a true Phyics Informed Neural\n",
    "        # Network, otherwise, it is just a data trained Neural network\n",
    "        self.loss = tf.reduce_sum(tf.square(self.u_tf   - self.u_pred)) /(u_norm**2) + \\\n",
    "                    tf.reduce_sum(tf.square(self.rho_tf - self.rho_pred)) /(rho_norm**2) + \\\n",
    "                    tf.reduce_sum(tf.square(self.p_tf   - self.p_pred)) /(p_norm**2) + \\\n",
    "                    tf.reduce_sum(tf.square(self.E_tf   - self.E_pred)) /(E_norm**2) + \\\n",
    "                    w*tf.reduce_sum(tf.square(self.e1))/(e1_norm**2) + \\\n",
    "                    w*tf.reduce_sum(tf.square(self.e2))/(e2_norm**2) + \\\n",
    "                    w*tf.reduce_sum(tf.square(self.e3))/(e3_norm**2) + \\\n",
    "                    w*tf.reduce_sum(tf.square(self.e4))/(p_norm**2)\n",
    "        i = 0\n",
    "        while i<48:\n",
    "          self.loss = self.loss + tf.reduce_sum(tf.square(self.n_tf[i] - self.n_pred[i]))/(n_norm[i]**2) + \\\n",
    "                                  tf.reduce_sum(tf.square(self.R_tf[i] - self.R_pred[i]))/(R_norm[i]**2) + \\\n",
    "                                  w*tf.reduce_sum(tf.square(self.en[i]))/(en_norm[i]**2)\n",
    "          i = i + 1\n",
    "\n",
    "        # Define optimizers\n",
    "        # There are 2 optimizers used: external by Scipy (L-BFGS-B) and internal\n",
    "        # by Tensorflow (which is Adam). The external optimizer gives an extra\n",
    "        # push after the internal has done its job. No need to change the default\n",
    "        # options of the optimizers. We have used Adam optimizer in this case,\n",
    "        # since, it is the most common and generally the fastest known converger\n",
    "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss,\n",
    "                                                                method = 'L-BFGS-B',\n",
    "                                                                options = {'maxiter': 50000,\n",
    "                                                                           'maxfun': 50000,\n",
    "                                                                           'maxcor': 50,\n",
    "                                                                           'maxls': 50,\n",
    "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
    "        # Adam\n",
    "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
    "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
    "\n",
    "        # Run the session after variable initialization\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    # Class methods\n",
    "\n",
    "    # These are basic initialization functions to create the weigths and biases\n",
    "    # tensor variables and assign random values to start with code snippet\n",
    "    # iterates over the layers vector to generate the tensors as stated\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "\n",
    "    # Code for a single forward propagation pass taking in weights, biases and\n",
    "    # input matrix X. Note the normalization step on X as H before passing on to\n",
    "    # the network\n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "\n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "    # This is the differentiating code snippet which does the Automatic\n",
    "    # Differential method to find the coefficients of the necessary gradients (or\n",
    "    # equivalently, residuals) to be used in the MSE (mean squared error) in the\n",
    "    # cost function. The step is to reference the earlier function neural_net and\n",
    "    # gain the outputs as a matrix. The matrix is then sliced into individual\n",
    "    # components to get pressure, density, speed and specific energy. We define\n",
    "    # the cross section area contour S to be used in the following Physical\n",
    "    # expressions for 1D Nozzle flow. Next we find the residuals by AutoDiff.\n",
    "    # The autodiff function provided by Tensorflow is tf.gradients as above. The\n",
    "    # mass_flow_grad, momentum_grad and energy_grad are actually the residuals of\n",
    "    # the three Compressible Physical expressions. Return all the variables back\n",
    "    # to the class attributes.\n",
    "    def net_Euler_STS(self, x):\n",
    "\n",
    "        nci_rho_u_p_E = self.neural_net(tf.concat([x], 1), self.weights, self.biases)\n",
    "\n",
    "        n = [nci_rho_u_p_E[:,i:i+1] for i in range(int(48))]\n",
    "\n",
    "        rho = nci_rho_u_p_E[:,48:49]\n",
    "        u   = nci_rho_u_p_E[:,49:50]\n",
    "        p   = nci_rho_u_p_E[:,50:51]\n",
    "        E   = nci_rho_u_p_E[:,51:52]\n",
    "\n",
    "        R = [nci_rho_u_p_E[:,i+52:i+53] for i in range(int(48))]\n",
    "\n",
    "        n_u_x = [tf.gradients(n[i] *u, x)[0] for i in range(int(48))]\n",
    "\n",
    "        # autodiff gradient #1\n",
    "        mass_flow_grad = tf.gradients(rho*u, x)[0]\n",
    "\n",
    "        # autodiff gradient #2\n",
    "        momentum_grad = tf.gradients((rho*u*u + p), x)[0]\n",
    "\n",
    "        # autodiff gradient #3\n",
    "        energy_grad = tf.gradients((rho*E + p)*u, x)[0]\n",
    "\n",
    "        # state residual\n",
    "        gamma = 1.4\n",
    "        state_res = p - rho*(gamma-1.0)*(E-0.5*gamma*u*u)\n",
    "\n",
    "        eqn = [n_u_x[i] - R[i] for i in range(int(48))]\n",
    "\n",
    "        eq1 =  mass_flow_grad\n",
    "        eq2 =  momentum_grad\n",
    "        eq3 =  energy_grad\n",
    "        eq4 =  state_res\n",
    "\n",
    "        return n, \\\n",
    "               rho, u, p, E, \\\n",
    "               R, \\\n",
    "               eq1, eq2, eq3, eq4, \\\n",
    "               eqn\n",
    "\n",
    "    # Callback method prints the current loss (cost) value of the NN\n",
    "    def callback(self, loss):\n",
    "        print('Loss: %.3e' % (loss))\n",
    "\n",
    "    def train(self, nIter):\n",
    "\n",
    "        tf_dict = {self.x_tf: self.x}\n",
    "        tf_dict.update(dict(zip(self.n_tf, self.n)))\n",
    "        tf_dict.update({self.rho_tf: self.rho, self.u_tf: self.u, self.p_tf: self.p, self.E_tf: self.E})\n",
    "        tf_dict.update(dict(zip(self.R_tf, self.R)))\n",
    "\n",
    "        global loss_vector\n",
    "        loss_vector = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        for it in range(nIter):\n",
    "            self.sess.run(self.train_op_Adam, tf_dict)\n",
    "\n",
    "            loss_value = self.sess.run(self.loss, tf_dict)\n",
    "            loss_vector.append(loss_value)\n",
    "\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                res1 = self.sess.run(self.e1, tf_dict)\n",
    "                res2 = self.sess.run(self.e2, tf_dict)\n",
    "                res3 = self.sess.run(self.e3, tf_dict)\n",
    "\n",
    "                print('Iter: %d, Loss: %.3e, Time: %.2f' %\n",
    "                      (it, loss_value, elapsed))\n",
    "                print('Mass Residual: %f\\t\\tMomentum Residual: %f\\tEnergy Residual: %f'\n",
    "                    %(sum(map(lambda a:a*a,res1))/len(res1), sum(map(lambda a:a*a,res2))/len(res2), sum(map(lambda a:a*a,res3))/len(res3)))\n",
    "                start_time = time.time()\n",
    "\n",
    "        # The following is external optimizer.\n",
    "        # Uncomment it to see in action. It runs indefinitely till\n",
    "        # convergence. Even after the iterations are finished, the optimizer\n",
    "        # continues to minimize the loss as compelled to by the statement\n",
    "        # self.optimizer.minimize, passing tf_dict to the loss expression defined\n",
    "        # as an attribute earlier in the class.\n",
    "        self.optimizer.minimize(self.sess,\n",
    "                feed_dict = tf_dict,\n",
    "                fetches = [self.loss],\n",
    "                loss_callback = self.callback)\n",
    "\n",
    "        return loss_vector\n",
    "\n",
    "    # Predict method is used to predict the output values when test data is\n",
    "    # passed into the network after the training is completed. All the values are\n",
    "    # returned to the call.\n",
    "    def predict(self, x_test):\n",
    "\n",
    "      tf_dict  = {self.x_tf: x_test}\n",
    "\n",
    "      n_test   = [self.sess.run(self.n_pred[i], tf_dict) for i in range(int(48))]\n",
    "      rho_test = self.sess.run(self.rho_pred, tf_dict)\n",
    "      u_test   = self.sess.run(self.u_pred, tf_dict)\n",
    "      p_test   = self.sess.run(self.p_pred, tf_dict)\n",
    "      E_test   = self.sess.run(self.E_pred, tf_dict)\n",
    "      R_test   = [self.sess.run(self.R_pred[i], tf_dict) for i in range(int(48))]\n",
    "\n",
    "      return n_test, rho_test, u_test, p_test, E_test, R_test\n",
    "\n",
    "\n",
    "    def plot_solution(X_star, u_star, index):\n",
    "\n",
    "      lb = X_star.min(0)\n",
    "      ub = X_star.max(0)\n",
    "\n",
    "      nn = 500\n",
    "\n",
    "      x = np.linspace(lb[0], ub[0], nn)\n",
    "      t = np.linspace(lb[1], ub[1], nn)\n",
    "      X, T = np.meshgrid(x,t)\n",
    "\n",
    "      U_star = griddata(X_star, u_star.flatten(), (X, T), method='cubic')\n",
    "\n",
    "      plt.figure(index)\n",
    "      plt.pcolor(X,T,U_star, cmap = 'jet')\n",
    "      plt.colorbar()\n",
    "      plt.show()\n",
    "\n",
    "# MAIN function\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # NN Architecture\n",
    "    layers = [1, 100, 100]\n",
    "    #layers = [1, 25, 50, 75, 50, 25, 100]\n",
    "\n",
    "    # Loading data\n",
    "    data = np.loadtxt(\"dataset_STS.txt\")\n",
    "\n",
    "    # Slicing of dataset to get all required parameters\n",
    "    x   = data[:,0:1].flatten()[:,None]\n",
    "    n   = [data[:,i:i+1].flatten()[:,None] for i in range(int(48))]\n",
    "    rho = data[:,49:50].flatten()[:,None]\n",
    "    u   = data[:,50:51].flatten()[:,None]\n",
    "    p   = data[:,51:52].flatten()[:,None]\n",
    "    E   = data[:,52:53].flatten()[:,None]\n",
    "    R   = [data[:,i+53:i+54].flatten()[:,None] for i in range(int(48))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = train_test_split(x, *n, rho, u, p, E, *R, test_size=0.15, random_state=0)\n",
    "    tts = np.array(tts)\n",
    "    x_train   = tts[0]\n",
    "    x_test    = tts[1]\n",
    "    rho_train = tts[98]\n",
    "    rho_test  = tts[99]\n",
    "    u_train   = tts[100]\n",
    "    u_test    = tts[101]\n",
    "    p_train   = tts[102]\n",
    "    p_test    = tts[103]\n",
    "    E_train   = tts[104]\n",
    "    E_test    = tts[105]\n",
    "    i = 0\n",
    "    j = 0\n",
    "    n_train = []\n",
    "    n_test  = []\n",
    "    R_train = []\n",
    "    R_test  = []\n",
    "    while i<48:\n",
    "      n_train.append(tts[j+2])\n",
    "      n_test.append(tts[j+3])\n",
    "      R_train.append(tts[j+106])\n",
    "      R_test.append(tts[j+107])\n",
    "      i = i + 1\n",
    "      j = j + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x = MinMaxScaler(); sc_x.fit(x_train); x_train = sc_x.transform(x_train)\n",
    "    sc_n = [MinMaxScaler() for i in range(int(48))]\n",
    "\n",
    "    i = 0\n",
    "    while i<48:\n",
    "      sc_n[i].fit(n_train[i])\n",
    "      n_train[i] = sc_n[i].transform(n_train[i])\n",
    "      i = i + 1\n",
    "\n",
    "    sc_rho = MinMaxScaler(); sc_rho.fit(rho_train); rho_train = sc_rho.transform(rho_train)\n",
    "    sc_u   = MinMaxScaler(); sc_u.fit(u_train)    ; u_train   = sc_u.transform(u_train)\n",
    "    sc_p   = MinMaxScaler(); sc_p.fit(p_train)    ; p_train   = sc_p.transform(p_train)\n",
    "    sc_E   = MinMaxScaler(); sc_E.fit(E_train)    ; E_train   = sc_E.transform(E_train)\n",
    "    sc_R   = [MinMaxScaler() for i in range(int(48))]\n",
    "\n",
    "    i = 0\n",
    "    while i<48:\n",
    "      sc_R[i].fit(R_train[i])\n",
    "      R_train[i] = sc_R[i].transform(R_train[i])\n",
    "      i = i + 1\n",
    "\n",
    "    x_test   = sc_x.transform(x_test)\n",
    "    n_test   = [sc_n[i].transform(n_test[i]) for i in range(int(48))]\n",
    "    rho_test = sc_rho.transform(rho_test)\n",
    "    u_test   = sc_u.transform(u_test)\n",
    "    p_test   = sc_p.transform(p_test)\n",
    "    E_test   = sc_E.transform(E_test)\n",
    "    R_test   = [sc_R[i].transform(R_test[i]) for i in range(int(48))]\n",
    "\n",
    "    # Training the NN\n",
    "    model = PINN(x_train,\n",
    "                 n_train,\n",
    "                 rho_train, u_train, p_train, E_train,\n",
    "                 R_train, layers)\n",
    "\n",
    "    model.train(1000)\n",
    "\n",
    "    # Plotting Loss\n",
    "    plt.plot(loss_vector, label='Loss value')\n",
    "    plt.legend()\n",
    "    plt.title('Loss value over iterations')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    #plt.savefig(f\"{images_dir}/Loss\", crop = False)\n",
    "\n",
    "    # Prediction\n",
    "    [n_pred, rho_pred, u_pred, p_pred, E_pred, R_pred] = model.predict(x_test)\n",
    "\n",
    "    # Normal relative error is printed for each variable\n",
    "    error_n   = [np.linalg.norm(n_test[i] - n_pred[i], 2)/np.linalg.norm(n_test[i],2) for i in range(int(48))]\n",
    "    error_rho = np.linalg.norm(rho_test-rho_pred,2)/np.linalg.norm(rho_test,2)\n",
    "    error_u   = np.linalg.norm(u_test-u_pred,2)/np.linalg.norm(u_test,2)\n",
    "    error_p   = np.linalg.norm(p_test-p_pred,2)/np.linalg.norm(p_test,2)\n",
    "    error_E   = np.linalg.norm(E_test-E_pred,2)/np.linalg.norm(E_test,2)\n",
    "    error_R   = [np.linalg.norm(R_test[i] - R_pred[i], 2)/np.linalg.norm(R_test[i],2) for i in range(int(48))]\n",
    "\n",
    "    i = 0\n",
    "    while i<48:\n",
    "        errorN = error_n[i]\n",
    "        errorR = error_R[i]\n",
    "        print(\"Test Error in nci: \"+str(errorN))\n",
    "        print(\"Test Error in Rci: \"+str(errorR))\n",
    "        i=i+1\n",
    "    #print(\"Test Error in nci: \"+str(error_n[i] for i in range(int(48))))\n",
    "    print(\"Test Error in rho: \"+str(error_rho))\n",
    "    print(\"Test Error in u: \"+str(error_u))\n",
    "    print(\"Test Error in p: \"+str(error_p))\n",
    "    print(\"Test Error in E: \"+str(error_E))\n",
    "    #print(\"Test Error in Rci: \"+str(error_R[i] for i in range(int(48))))\n",
    "\n",
    "    # inverse transform\n",
    "    x_train_sb   = sc_x.inverse_transform(x_train)\n",
    "    n_train_sb   = [sc_n[i].inverse_transform(n_train[i]) for i in range(int(48))]\n",
    "    rho_train_sb = sc_rho.inverse_transform(rho_train)\n",
    "    u_train_sb   = sc_u.inverse_transform(u_train)\n",
    "    p_train_sb   = sc_p.inverse_transform(p_train)\n",
    "    E_train_sb   = sc_E.inverse_transform(E_train)\n",
    "    R_train_sb   = [sc_R[i].inverse_transform(R_train[i]) for i in range(int(48))]\n",
    "    x_test_sb    = sc_x.inverse_transform(x_test)\n",
    "    n_test_sb    = [sc_n[i].inverse_transform(n_test[i]) for i in range(int(48))]\n",
    "    rho_test_sb  = sc_rho.inverse_transform(rho_test)\n",
    "    u_test_sb    = sc_u.inverse_transform(u_test)\n",
    "    p_test_sb    = sc_p.inverse_transform(p_test)\n",
    "    E_test_sb    = sc_E.inverse_transform(E_test)\n",
    "    R_test_sb    = [sc_R[i].inverse_transform(R_test[i]) for i in range(int(48))]\n",
    "    n_pred_sb    = [sc_n[i].inverse_transform(n_pred[i]) for i in range(int(48))]\n",
    "    rho_pred_sb  = sc_rho.inverse_transform(rho_pred)\n",
    "    u_pred_sb    = sc_u.inverse_transform(u_pred)\n",
    "    p_pred_sb    = sc_p.inverse_transform(p_pred)\n",
    "    E_pred_sb    = sc_E.inverse_transform(E_pred)\n",
    "    R_pred_sb    = [sc_R[i].inverse_transform(R_pred[i]) for i in range(int(48))]\n",
    "\n",
    "    # Plot Nci\n",
    "    plt.plot(x_test_sb, n_pred_sb[2], 'o', color='black', label='NN, i=3', linewidth=4, markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, n_test_sb[2], 'o', color='red',   label='Exact, i=3', markersize=4)\n",
    "    plt.plot(x_test_sb, n_pred_sb[5], 'o', color='black', label='NN, i=6', linewidth=4, markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, n_test_sb[5], 'o', color='blue',   label='Exact, i=6', markersize=4)\n",
    "    plt.plot(x_test_sb, n_pred_sb[8], 'o', color='black', label='NN, i=9', linewidth=4, markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, n_test_sb[8], 'o', color='green',   label='Exact, i=9', markersize=4)\n",
    "    plt.plot(x_test_sb, n_pred_sb[11], 'o', color='black', label='NN, i=12', linewidth=4, markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, n_test_sb[11], 'o', color='magenta',   label='Exact, i=12', markersize=4)\n",
    "    plt.plot(x_test_sb, n_pred_sb[14], 'o', color='black', label='NN, i=15', linewidth=4, markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, n_test_sb[14], 'o', color='yellow',   label='Exact, i=15', markersize=4)\n",
    "    #plt.title('Comparison of NN and Exact solution for Molecular Number Density')\n",
    "    plt.xlabel('X [mm]')\n",
    "    plt.ylabel('$n_{ci}$ $[m^-3]$')\n",
    "    #plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{images_dir}/Nci\", crop='false')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Nat\n",
    "    plt.plot(x_test_sb, n_pred_sb[47], 'o', color='black', label='NN', linewidth=2, markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, n_test_sb[47], 'o', color='red',   label='Exact', markersize=4)\n",
    "    #plt.title('Comparison of NN and Exact solution for Atomic Number Density')\n",
    "    plt.xlabel('X [mm]')\n",
    "    plt.ylabel('$n_{at}$ $[m^-3]$')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{images_dir}/Nat\", crop='false')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot RHO\n",
    "    plt.plot(x_test_sb, rho_pred_sb, 'o', color='black', label='NN', linewidth=2, markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, rho_test_sb, 'o', color='red',   label='Exact', markersize=4)\n",
    "    #plt.title('Comparison of NN and Exact solution for Density')\n",
    "    plt.xlabel('X [mm]')\n",
    "    plt.ylabel(r'$\\rho$ $[kg/m^3]$')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{images_dir}/RHO\", crop='false')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot P\n",
    "    plt.plot(x_test_sb, p_pred_sb, 'o', color='black', label='NN', linewidth=2, markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, p_test_sb, 'o', color='red',   label='Exact', markersize=4)\n",
    "    #plt.title('Comparison of NN and Exact solution for Pressure')\n",
    "    plt.xlabel('X [mm]')\n",
    "    plt.ylabel('P [Pa]')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{images_dir}/P\", crop='false')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot U\n",
    "    plt.plot(x_test_sb, u_pred_sb, 'o', color='black', label='NN', linewidth=2, markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, u_test_sb, 'o', color='red',   label='Exact', markersize=4)\n",
    "    #plt.title('Comparison of NN and Exact solution for Velocity')\n",
    "    plt.xlabel('X [mm]')\n",
    "    plt.ylabel('U [m/s]')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{images_dir}/U\", crop='false')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot E\n",
    "    plt.plot(x_test_sb, E_pred_sb, 'o', color='black', label='NN', linewidth=2, markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, E_test_sb, 'o', color='red',   label='Exact', markersize=4)\n",
    "    #plt.title('Comparison of NN and Exact solution for Energy')\n",
    "    plt.xlabel('X [mm]')\n",
    "    plt.ylabel('E [eV]')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{images_dir}/E\", crop='false')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Rci\n",
    "    plt.plot(x_test_sb, R_pred_sb[2],  'o', color='black',   label='NN, i=3',  linewidth=4,  markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, R_test_sb[2],  'o', color='red',     label='Exact, i=3',            markersize=4)\n",
    "    plt.plot(x_test_sb, R_pred_sb[5],  'o', color='black',   label='NN, i=6',  linewidth=4,  markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, R_test_sb[5],  'o', color='blue',    label='Exact, i=6',            markersize=4)\n",
    "    plt.plot(x_test_sb, R_pred_sb[8],  'o', color='black',   label='NN, i=9',  linewidth=4,  markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, R_test_sb[8],  'o', color='green',   label='Exact, i=9',            markersize=4)\n",
    "    plt.plot(x_test_sb, R_pred_sb[11], 'o', color='black',   label='NN, i=12', linewidth=4, markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, R_test_sb[11], 'o', color='magenta', label='Exact, i=12',           markersize=4)\n",
    "    plt.plot(x_test_sb, R_pred_sb[14], 'o', color='black',   label='NN, i=15', linewidth=4, markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, R_test_sb[14], 'o', color='yellow',  label='Exact, i=15',           markersize=4)\n",
    "    #plt.title('Comparison of NN and Exact solution for $R_{ci}$')\n",
    "    plt.xlabel('X [mm]')\n",
    "    plt.ylabel(r'$R_{ci} [J/m^3/s]$')\n",
    "    #plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{images_dir}/Rci\", crop='false')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Rat\n",
    "    plt.plot(x_test_sb, R_pred_sb[47], 'o', color='black', label='NN', linewidth=2, markersize=5, fillstyle='none')\n",
    "    plt.plot(x_test_sb, R_test_sb[47], 'o', color='red',   label='Exact',           markersize=4 )\n",
    "    #plt.title('Comparison of NN and Exact solution for $R_{at}$')\n",
    "    plt.xlabel('X [mm]')\n",
    "    plt.ylabel(r'$R_{at} [J/m^3/s]$')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{images_dir}/Rat\", crop='false')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
